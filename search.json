[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Embedded Systems Design",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Embedded Systems Design",
    "section": "Welcome",
    "text": "Welcome\nWelcome to the “System Design with Silicon Lab EFR32XG24 BLE Microcontroller”. This book is designed to guide you through the process of programming, building applications, and integrating machine learning with the EFR32XG24 BLE Microcontroller. Whether you’re an engineering student or a seasoned professional, this book offers hands-on examples to make advanced concepts accessible.\nYou’ll learn how to: - Program the EFR32XG24 microcontroller using C. - Design and implement embedded systems applications. - Apply machine learning techniques to solve real-world problems. - Explore gesture recognition, anomaly detection, and audio-based ML solutions.\nThe book balances theory with practice, empowering readers to develop embedded systems that are robust, efficient, and intelligent.\nIf you’re interested in broader programming concepts or other machine learning platforms, we encourage you to explore additional resources and apply your learning across domains.\n\n\n\n\n\n\nThis book was originally developed as part of the EE260 and EE513 courses at Clarkson University. The Quarto-based version serves as an example of modern technical publishing and open access education.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Embedded Systems Design",
    "section": "License",
    "text": "License\nThis book is free to use under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are welcome to share, adapt, and use the material for educational purposes, as long as proper attribution is given and no commercial use is made.\nIf you’d like to support the project or contribute, you can report issues or submit pull requests at github.com/clarkson-edge/ee513_book. Thank you for helping improve this resource for the community.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/core/1_introduction.html",
    "href": "contents/core/1_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Overview\nEmbedded systems are specialized computing systems that are designed to perform dedicated functions or tasks within a larger mechanical or electrical system. Unlike general-purpose computers, embedded systems are optimized for specific applications, balancing constraints such as power consumption, real-time performance, and cost efficiency. They are integral to a wide range of applications, including consumer electronics, automotive systems, medical devices, industrial automation, and smart home technologies.\nAt the core of most embedded systems lies a microcontroller, a compact integrated circuit that combines a processor, memory, and input/output peripherals on a single chip. Microcontrollers are the brain of embedded systems, executing pre-programmed instructions to manage sensors, actuators, and communication modules. Their efficiency, reliability, and low power consumption make them ideal for embedded applications.\nIn recent years, the demand for wireless communication in embedded systems has surged, driven by the growth of the Internet of Things (IoT). Among the various wireless protocols, Bluetooth Low Energy (BLE) has emerged as a key technology for low-power, short-range communication. BLE enables devices to transmit small amounts of data with minimal energy consumption, making it ideal for battery-operated applications such as fitness trackers, smart home devices, and health monitoring systems.\nThe Silicon Labs EFR32XG24 series is one of the most advanced BLE microcontrollers available in Q4 of 2024. Built on the ARM Cortex-M33 core, it offers a powerful blend of performance, energy efficiency, and wireless connectivity. It is equipped with a robust BLE stack, extensive peripherals, and advanced security features, making it a preferred choice for designing sophisticated embedded systems.\nThis textbook, System Design with Silicon Lab EFR32XG24 BLE Microcontroller, is intended to provide a guide for students and engineers to understand and design embedded systems using the EFR32xG24 Dev Kit. The book covers both theoretical concepts and hands-on practical implementations, ensuring readers gain a deep understanding of embedded system design and BLE communication protocols.\nThroughout this book, readers will learn:",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/1_introduction.html#overview",
    "href": "contents/core/1_introduction.html#overview",
    "title": "1  Introduction",
    "section": "",
    "text": "The fundamentals of embedded systems and microcontroller architecture.\nKey features and capabilities of the EFR32XG24 BLE microcontroller.\nPractical techniques for programming and debugging embedded systems.\nBLE communication protocols and integration with IoT applications.\nReal-world case studies and projects demonstrating system design principles.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/1_introduction.html#real-world-applications-of-embedded-systems",
    "href": "contents/core/1_introduction.html#real-world-applications-of-embedded-systems",
    "title": "1  Introduction",
    "section": "1.2 Real-World Applications of Embedded Systems",
    "text": "1.2 Real-World Applications of Embedded Systems\nEmbedded systems are deeply integrated into modern life, serving as the backbone for countless devices and technologies. They are designed to execute dedicated functions efficiently while operating under constraints such as power consumption, memory limitations, and cost. Examples of embedded systems can be observed in both everyday objects and complex industrial applications, showcasing their versatility and importance in modern engineering.\nOne prominent example is the automotive industry, where embedded systems play a critical role in ensuring safety, efficiency, and advanced functionalities. Anti-lock Braking Systems (ABS) use embedded controllers to regulate brake pressure, preventing skidding on slippery roads and enhancing vehicle stability. Similarly, adaptive cruise control systems utilize embedded microcontrollers to monitor vehicle speed and distance through RADAR or LIDAR sensors, enabling intelligent speed adjustments. Another safety-critical application is the airbag control system, which relies on real-time sensor data to trigger airbag deployment within milliseconds during a collision.\nIn industrial automation, embedded systems are central to the operation of robotic arms, conveyor belts, and assembly lines. These systems handle precise sequencing, closed-loop control, and real-time signal processing to maintain efficiency and safety. For example, industrial robots are programmed to carry out repetitive tasks such as welding, painting, and packaging, each controlled by embedded microcontrollers to ensure accuracy and reliability.\nConsumer electronics also heavily rely on embedded systems. Devices such as programmable engineering calculators, automated teller machines (ATMs), and smart home appliances incorporate microcontrollers to perform specific tasks seamlessly. Modern washing machines, for instance, utilize embedded controllers to monitor water levels, manage wash cycles, and adjust spin speeds dynamically. Similarly, ATMs use embedded microcontrollers to process transactions securely while managing input and output operations.\nBLE microcontrollers have further extended the capabilities of embedded systems by enabling low-power wireless communication. BLE technology is particularly advantageous in battery-operated devices like fitness trackers, smart home sensors, and medical monitoring equipment. These microcontrollers facilitate energy-efficient data transmission, allowing devices to remain functional for extended periods without frequent battery replacements.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/1_introduction.html#overview-of-efr32mg24-microcontroller",
    "href": "contents/core/1_introduction.html#overview-of-efr32mg24-microcontroller",
    "title": "1  Introduction",
    "section": "1.3 Overview of EFR32MG24 Microcontroller",
    "text": "1.3 Overview of EFR32MG24 Microcontroller\nThe EFR32MG24 microcontroller, part of Silicon Labs’ Wireless Gecko series, is specifically designed to address the growing demand for energy-efficient and high-performance wireless communication in embedded systems. Built on the ARM Cortex-M33 core, it operates at a maximum frequency of 78 MHz, delivering sufficient computational power for real-time applications while maintaining energy efficiency. The microcontroller integrates advanced hardware security features, including a hardware cryptographic accelerator and Secure Boot, ensuring robust protection against cyber threats. It supports multiple wireless protocols, with a primary focus on BLE 5.3, enabling reliable, low-power, short-range communication. The integrated radio transceiver offers industry-leading sensitivity and output power, ensuring stable connectivity even in challenging environments.\nThe EFR32MG24 is equipped with a range of analog and digital peripherals, including ADCs, DACs, timers, UART, SPI, and I2C interfaces, providing flexible options for sensor integration and peripheral control. Its low-power modes, combined with energy-saving peripherals and Sleep Timer capabilities, make it highly suitable for battery-operated devices such as IoT sensor nodes, wearable electronics, and smart home devices. This also features an on-chip AI/ML hardware accelerator, enabling edge-computing capabilities for tasks like sensor data analysis and anomaly detection. Hence, this microcontroller, available in the xG24-DK2601B Development Kit, is chosen for this book.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html",
    "href": "contents/core/2_programmingwithc.html",
    "title": "2  Programming Embedded Systems with C",
    "section": "",
    "text": "2.1 Simplicity Studio IDE for Silicon Labs EFR32XG24 Microcontroller\nSimplicity Studio is the official Integrated Development Environment (IDE) provided by Silicon Labs for embedded development with their microcontrollers, including the EFR32XG24 BLE microcontroller to be covered in this textbook. It is a feature-rich platform designed to streamline the development process, offering a library of example projects, application templates, and related tools for writing, debugging, profiling, and deploying firmware applications efficiently. It integrates multiple tools into one unified interface:\nSimplicity Studio supports a range of compilers tailored for embedded systems:\nFor the EFR32XG24 microcontroller, GCC is the default compiler bundled with Simplicity Studio, offering robust optimization and compatibility with ARM Cortex-M33 cores.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#simplicity-studio-ide-for-silicon-labs-efr32xg24-microcontroller",
    "href": "contents/core/2_programmingwithc.html#simplicity-studio-ide-for-silicon-labs-efr32xg24-microcontroller",
    "title": "2  Programming Embedded Systems with C",
    "section": "",
    "text": "Project Management: Create, organize, and manage embedded projects.\nDevice Configuration: Configure peripheral modules and optimize hardware settings.\nDebugging Tools: Real-time debugging with SEGGER J-Link integration.\nEnergy Profiler: Monitor and optimize power consumption of embedded applications.\nWireless Network Analyzer: Analyze wireless traffic and optimize communication protocols.\n\n\n\nGCC (GNU Compiler Collection): Open-source compiler widely used in embedded systems.\nIAR Embedded Workbench Compiler: Commercial compiler known for its optimization capabilities.\nKeil ARM Compiler (ARMCC): Industry-standard compiler for ARM Cortex-M series microcontrollers.\n\n\n\n2.1.1 Development Workflow in Simplicity Studio\nThe typical workflow when using Simplicity Studio for EFR32XG24 development involves:\n\nDevice Selection: Select the target microcontroller (EFR32XG24) from the device catalog.\nProject Creation: Use templates or start from scratch to create firmware projects.\nPeripheral Configuration: Use the graphical configuration tool to set up GPIO, timers, UART, SPI, etc.\nCode Generation: Auto-generate initialization code based on configuration settings.\nBuild and Compile: Compile code using GCC or other selected compilers.\nDebug and Test: Use SEGGER J-Link debugger for step-by-step debugging and breakpoint management.\nEnergy Profiling: Use the energy profiler to optimize power consumption.\n\n\n\n2.1.2 Key Features of Simplicity Studio for EFR32XG24\nThe Graphical Peripheral Configuration Tool provides an intuitive interface for configuring peripherals and pin assignments, reducing setup errors. The Real-Time Energy Profiler enables precise monitoring and analysis of energy consumption, helping developers optimize power efficiency. The Wireless Network Analyzer facilitates debugging and fine-tuning of Bluetooth communication channels, ensuring reliable wireless connectivity. Additionally, Simplicity Studio includes SDK Integration with pre-built libraries and frameworks for BLE and IoT applications. Developers can also leverage an Extensive Example Codebase, which contains numerous pre-written projects for rapid prototyping and reduced development time.\nTo maximize productivity and ensure reliable outcomes, developers should follow established best practices when using Simplicity Studio. It is essential to keep the IDE updated to the latest version to benefit from bug fixes and new features. The graphical configuration tools should be used whenever possible to minimize errors during peripheral setup. Compiler optimizations should be enabled to account for the resource-constrained nature of embedded environments. Regular energy profiling should be conducted throughout firmware development to identify and address power inefficiencies. Lastly, developers should use the SEGGER J-Link Debugger for precise, real-time debugging and analysis of embedded applications.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#structure-of-an-embedded-c-program",
    "href": "contents/core/2_programmingwithc.html#structure-of-an-embedded-c-program",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.2 Structure of an Embedded C Program",
    "text": "2.2 Structure of an Embedded C Program\nA typical embedded C program follows a standardized structure to maintain clarity, modularity, and efficient hardware interaction. A common format that is found in Arduino IDE is as follows:\n#include &lt;stdint.h&gt;\n#define LED_PIN 13\n\nvoid init();\nvoid loop();\n\nint main() {\n    init();\n    while (1) {\n        loop();\n    }\n}\n\nvoid init() {\n    // Initialization code\n}\n\nvoid loop() {\n    // Main functionality code\n}\nAt the core lies the main() function, which serves as the entry point for program execution. The program begins with an init() function, responsible for hardware and peripheral initialization, such as configuring GPIO pins, timers, and communication interfaces. Following initialization, the program enters an infinite while(1) loop, where the loop() function is repeatedly called to handle the system’s primary tasks. This structure separates setup and runtime logic, promoting code readability and easier debugging. The use of #include &lt;stdint.h&gt; ensures access to fixed-width integer types, while the #define LED_PIN 13 macro simplifies hardware pin configuration. This modular design allows embedded systems to maintain deterministic behavior.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#structure-of-an-embedded-c-program-in-simplicity-studio",
    "href": "contents/core/2_programmingwithc.html#structure-of-an-embedded-c-program-in-simplicity-studio",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.3 Structure of an Embedded C Program in Simplicity Studio",
    "text": "2.3 Structure of an Embedded C Program in Simplicity Studio\nIn Simplicity Studio, an embedded C program adheres to a standardized structure designed to ensure modularity, hardware abstraction, and efficient execution on microcontrollers like the EFR32XG24. A typical program format is shown below:\n#include \"em_device.h\"\n#include \"em_chip.h\"\n#include \"em_gpio.h\"\n\n#define LED_PIN 13\n\nvoid init();\nvoid loop();\n\nint main(void) {\n    CHIP_Init(); // Initialize the microcontroller system\n    init();\n    while (1) {\n        loop();\n    }\n}\n\nvoid init() {\n    // GPIO and peripheral initialization code\n}\n\nvoid loop() {\n    // Main functionality code\n}\nIn Simplicity Studio, the CHIP_Init() function is typically called at the beginning of the main() function to configure essential hardware components, including the clock management unit (CMU) and device-specific registers. The init() function follows, serving to initialize peripherals, configure GPIO pins, and set up timers or communication interfaces. The program then enters an infinite while(1) loop, where the loop() function repeatedly executes core tasks. Header files such as em_device.h provide device-specific definitions, while em_chip.h ensures system-level configurations are applied. The use of predefined macros like #define LED_PIN 13 simplifies hardware abstraction, improving code clarity and reducing errors. This structure leverages Simplicity Studio’s hardware abstraction layer (HAL) to provide a consistent programming interface, ensuring scalability and portability across Silicon Labs microcontroller families.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#generic-data-types-in-embedded-systems",
    "href": "contents/core/2_programmingwithc.html#generic-data-types-in-embedded-systems",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.4 Generic Data Types in Embedded Systems",
    "text": "2.4 Generic Data Types in Embedded Systems\nData types in embedded systems are carefully chosen based on performance requirements, memory constraints, and application-specific needs. Common data types include:\n\nInteger Data Types (ISO C99 Standard)\n\nint: Standard integer type, typically 16 or 32 bits depending on the microcontroller.\nuint8_t, uint16_t, uint32_t: Unsigned integer types offering precise control over memory usage.\nint8_t, int16_t, int32_t: Signed integer types for representing both positive and negative values.\n\n\n\nFloating-Point Data Types (IEEE 754 Standard)\n\nfloat: 32-bit floating-point type for representing decimal values.\ndouble: 64-bit floating-point type for higher precision.\n\nA summary of these types is displayed in Table 2.1.\nThe EFR32XG24 microcontroller includes an FPU (Floating-Point Unit) to handle floating-point calculations, but such operations can introduce performance and power efficiency overhead. Therefore, floating-point types should be used sparingly in embedded applications.\n\n\nCommonly used integer types when programming embedded systems\n\n\n\n\n\n\n\n\nData type\nSize\nRange min\nRange max\n\n\n\n\nint8_t\n8 bits (1 byte)\n-128\n127\n\n\nuint8_t\n8 bits (1 byte)\n0\n255\n\n\nint16_t\n16 bits (2 bytes)\n-32768\n32767\n\n\nuint16_t\n16 bits (2 bytes)\n0\n65535\n\n\nint32_t\n32 bits (4 bytes)\n-2,147,483,648\n2,147,483,647\n\n\nuint32_t\n32 bits (4 bytes)\n0\n4,294,967,295\n\n\nint64_t\n64 bits (8 bytes)\n-9,223,372,036,854,775,808\n9,223,372,036,854,775,807\n\n\nuint64_t\n64 bits (8 bytes)\n0\n18,446,744,073,709,551,615",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#choosing-the-right-data-type",
    "href": "contents/core/2_programmingwithc.html#choosing-the-right-data-type",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.5 Choosing the Right Data Type",
    "text": "2.5 Choosing the Right Data Type\nSelecting an appropriate data type in embedded systems programming is a crucial step to ensure optimal memory usage, computational efficiency, and prevention of data-related errors. The choice of data type must balance several key factors, including:\n\nPerformance: Larger data types consume more memory and require longer processing times.\nOverflow: Variables must be chosen to prevent exceeding their maximum allowable value.\nCoercion and Truncation: Automatic type conversion can lead to unintended behavior if not managed carefully.\n\nIn embedded systems, memory is a scarce resource, and improper data type selection can lead to unnecessary overhead. For example:\n\nOn an 8-bit microcontroller, using a 4-byte int instead of a 1-byte char for a simple counter wastes memory and processing cycles. For example,\nint counter = 0; // Uses 4 bytes unnecessarily on an 8-bit system\nuint8_t counter = 0; // Optimized for an 8-bit system\nOn a 32-bit microcontroller like the ARM Cortex-M, memory access is optimized for 4-byte alignment, and using smaller data types may not yield significant performance improvements.\n\n\nHandling Overflow\nOverflow occurs when a variable exceeds the maximum value that can be stored in its data type. In embedded systems, overflow can lead to unpredictable behavior or silent data corruption. For example:\nuint8_t seconds = 255; \nseconds += 1; // Overflow occurs, seconds resets to 0\nTo prevent overflow:\n\nUse larger data types if overflow is anticipated.\nImplement overflow detection mechanisms.\n\n\n\nData Coercion and Truncation\nIn embedded C, implicit type conversions (coercion) and truncation can lead to unintended results:\n\nWhen a smaller data type is promoted to a larger data type (e.g., uint8_t to uint16_t), padding may occur. For example,\nuint8_t smallValue = 200;\nuint16_t largeValue = smallValue; // Coercion from 8-bit to 16-bit\nWhen a larger data type is truncated to a smaller one (e.g., uint16_t to uint8_t), significant data loss may occur. For example,\nuint16_t largeValue = 1025;\nuint8_t smallValue = largeValue; // Truncation, smallValue = 1\n\n\n\nBest Practices for Data Type Selection\n\nUse fixed-width integer types from the stdint.h library (int8_t, uint16_t, etc.).\nAvoid mixing signed and unsigned data types in arithmetic operations.\nBe explicit in type casting and ensure expected results are validated.\nAlways check compiler warnings related to type conversions.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#memory-alignment-in-embedded-systems",
    "href": "contents/core/2_programmingwithc.html#memory-alignment-in-embedded-systems",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.6 Memory Alignment in Embedded Systems",
    "text": "2.6 Memory Alignment in Embedded Systems\nEfficient memory alignment is critical in embedded systems to optimize performance, reduce access latency, and ensure compatibility with the processor’s architecture. In microcontrollers like the EFR32XG24, unaligned memory access can lead to performance penalties or even cause system faults on certain architectures.\n\nUnderstanding Memory Alignment\n\nAligned Access: Data is stored in memory at addresses that are multiples of its size. For example:\n\nA 2-byte short int should be stored at an address divisible by\n\n\n\nA 4-byte int or float should be stored at an address divisible by 4.\n\nUnaligned Access: Data is stored at an address that does not adhere to its size requirements. For example:\n\nStoring a 4-byte int at an address like 0x20000003 is considered unaligned.\n\n\nAligned access is preferred because microcontrollers fetch data in word-sized chunks (e.g., 4 bytes on ARM Cortex-M processors). Misaligned data may require multiple memory accesses, increasing latency and power consumption.\n\n\nExample of Memory Alignment\nAligned Memory Example (Efficient Access):\nunsigned char a;       // 1-byte aligned at 0x20000000\nunsigned short b;      // 2-byte aligned at 0x20000002\nunsigned int c;        // 4-byte aligned at 0x20000004\nUnaligned Memory Example (Potential Performance Penalty):\nunsigned char a;       // Stored at 0x20000000\nunsigned short b;      // Stored at 0x20000001 (misaligned)\nunsigned int c;        // Stored at 0x20000003 (misaligned)\nAligned Memory Example (Efficient Access with Padding):\nunsigned char a;       // Stored at 0x20000000\nunsigned char padding; // Added for alignment\nunsigned short b;      // Stored at 0x20000002\nunsigned int c;        // Stored at 0x20000004\n\n\nBest Practices for Memory Alignment\n\nUse compiler directives or attributes to enforce memory alignment.\nGroup variables by size (e.g., group all char, then short, then int variables).\nAvoid unaligned data structures in performance-critical paths.\n\nCompiler Attribute Example (ARM GCC/Keil):\nstruct __attribute__((aligned(4))) AlignedStruct {\n    uint8_t a;\n    uint16_t b;\n    uint32_t c;\n};\nEfficient memory alignment reduces CPU cycles for memory fetches and avoids unnecessary overhead, making it a critical practice in embedded systems programming.\n\n\n2.6.1 Bitwise Operations\nBitwise operators are essential in embedded systems for manipulating hardware registers and performing efficient computations. These operations are fundamental for efficiently interacting with GPIO (General Purpose Input Output) registers in embedded systems. They allow precise control over individual bits, enabling configuration, status checking, and manipulation of GPIO pins without affecting other bits in the register.\nAND (&): Used for masking bits.\nOR (|): Used for setting bits.\nXOR (^): Used for toggling bits.\nNOT (~): Used for inverting bits.\nExample:\nuint8_t reg = 0b00001111;\nreg |= (1 &lt;&lt; 4); // Set the 5th bit\nreg &= ~(1 &lt;&lt; 2); // Clear the 3rd bit\n\n\n2.6.2 Bitwise Shift Operations\nShift operators move bits left (&lt;&lt;) or right (&gt;&gt;) and are commonly used for:\n\nMultiplying or dividing numbers by powers of two.\nSetting or clearing specific bits.\n\nExample:\nuint8_t value = 0x01;\nvalue &lt;&lt;= 3; // Shift left by 3 bits (Result: 0x08)\n\n\n2.6.3 Atomic Register Usage for GPIO Control\nAtomic GPIO operations are critical in embedded systems where precise and thread-safe pin manipulation is required. Unlike standard DOUT register operations (data directly outputs to pin), atomic registers (SET, CLR, and TGL) allow direct modification of specific bits without requiring a read-modify-write cycle.\n\nAdvantages of Atomic GPIO Operations\n\nThread-Safe: Prevents unintended side effects in concurrent operations.\nEfficient: Eliminates the overhead of read-modify-write cycles.\nPrecision: Ensures only the target bit is modified.\n\n\n\nKey Atomic Registers\n\nSET Register: Sets specific GPIO pins to a high state without affecting others.\nCLR Register: Clears specific GPIO pins to a low state without affecting others.\nTGL Register: Toggles specific GPIO pins without affecting others.\n\n\n\nAtomic Operations Examples (Explanation in Section 2.6)\n1. Setting Multiple Pins Atomically:\nGPIO-&gt;P_SET[gpioPortB].DOUT = (1 &lt;&lt; 2) | (1 &lt;&lt; 4); // Set pins 2 and 4 on Port B\n2. Clearing Specific Pins Atomically:\nGPIO-&gt;P_CLR[gpioPortC].DOUT = (1 &lt;&lt; 5); // Clear pin 5 on Port C\n3. Toggling Multiple Pins Atomically:\nGPIO-&gt;P_TGL[gpioPortD].DOUT = (1 &lt;&lt; 1) | (1 &lt;&lt; 3); // Toggle pins 1 and 3 on Port D\n\n\nAvoiding Race Conditions in GPIO Control\nIn real-time systems, race conditions can occur when multiple threads or interrupt routines attempt to modify GPIO pins simultaneously. Atomic registers mitigate this risk by ensuring:\n\nOnly the targeted pins are modified.\nNo unintended overwrites occur during concurrent access.\n\nExample of Thread-Safe Pin Toggle:\nvoid toggleLedThreadSafe(void) {\n    GPIO-&gt;P_TGL[gpioPortA].DOUT = (1 &lt;&lt; 6); // Safely toggle pin 6\n}\n\n\nBest Practices for Using Atomic Registers\n\nPrefer atomic registers for time-critical pin operations.\nAvoid mixing standard DOUT operations with atomic operations on the same pins.\nDocument atomic operations in shared resources clearly.\nTest interrupt-driven routines for predictable behavior with atomic GPIO controls.\n\n\n\nChecking the State of a GPIO Pin:\nuint8_t pinState = (GPIO-&gt;P[gpioPortA].DIN &gt;&gt; 3) & 1; // Read state of pin 3\n\n\nUsing Shift Operators for Pin Masking\nShift operators are commonly used to create masks for setting, clearing, or toggling specific bits in GPIO registers.\nExample - Setting Multiple Pins:\nGPIO-&gt;P[gpioPortA].DOUT |= (1 &lt;&lt; 3) | (1 &lt;&lt; 5); // Set pins 3 and 5\nExample - Clearing Multiple Pins:\nGPIO-&gt;P[gpioPortA].DOUT &= ~((1 &lt;&lt; 3) | (1 &lt;&lt; 5)); // Clear pins 3 and 5\n\n\nPractical Example: Blinking an LED Using Bitwise Operations\nThe following example demonstrates how to blink an LED connected to Port D, Pin 2 using bitwise operations:\n#define LED_PIN 2\n\n// Configure pin as output\nGPIO-&gt;P[gpioPortD].MODEL |= (1 &lt;&lt; (4 * LED_PIN)); // MODEL is the MODE Low register\n\n// Toggle the LED state in a loop\nwhile (1) {\n    GPIO-&gt;P[gpioPortD].DOUT ^= (1 &lt;&lt; LED_PIN); // Toggle LED pin\n    delay(1000); // 1-second delay\n}\n\n\nBest Practices for Bitwise GPIO Operations\n\nAlways mask the specific bits you intend to modify.\nAvoid direct assignments to GPIO registers; prefer bitwise operations.\nUse clear and descriptive macros for pin numbers and masks.\nTest configurations thoroughly to prevent accidental overwrites.\n\nBitwise operations provide low-level control over GPIO registers, ensuring efficient and predictable manipulation of hardware pins. Mastering these operations is essential for embedded systems programming.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#understanding-the---operator-in-atomic-gpio-operations",
    "href": "contents/core/2_programmingwithc.html#understanding-the---operator-in-atomic-gpio-operations",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.7 Understanding the -> Operator in Atomic GPIO Operations",
    "text": "2.7 Understanding the -&gt; Operator in Atomic GPIO Operations\nIn embedded systems programming, especially when interfacing with hardware peripherals such as GPIO registers, it is common to encounter expressions utilizing the -&gt; operator. This operator is used to access members of a structure through a pointer. In the context of atomic GPIO operations with the Silicon Labs EFR32XG24 microcontroller, the -&gt; operator simplifies hardware register access and enhance code clarity.\n\n2.7.1 Pointer to Structure and the -&gt; Operator\nIn C, the -&gt; operator is used to access a member of a structure when the structure is referred to by a pointer. The syntax is:\npointer-&gt;member\nThis is equivalent to:\n(*pointer).member\nHere:\n\npointer: Points to a structure (e.g., GPIO peripheral base address).\nmember: Represents a specific field in the structure (e.g., registers like P_SET, P_CLR, P_TGL).\n\n\n\n2.7.2 GPIO Structure and Enums in EFR32XG24\nThe GPIO peripheral on the EFR32XG24 microcontroller is represented as a structure, typically defined in the hardware abstraction layer (HAL). For example:\ntypedef struct {\n    volatile uint32_t DOUT;\n    volatile uint32_t SET;\n    volatile uint32_t CLR;\n    volatile uint32_t TGL;\n} GPIO_Port_TypeDef;\nAdditionally, GPIO ports are often enumerated for easy reference:\ntypedef enum {\n    gpioPortA,\n    gpioPortB,\n    gpioPortC,\n    gpioPortD\n} GPIO_Port_Type;\n\n\n2.7.3 Atomic GPIO Operations with -&gt;\nWhen performing atomic GPIO operations, the structure pointer enables access to specific GPIO port registers. For example:\nGPIO-&gt;P_SET[gpioPortB].DOUT = (1 &lt;&lt; 2) | (1 &lt;&lt; 4);\nExplanation:\n\nGPIO: Base pointer to the GPIO peripheral structure.\nP_SET: Array of registers representing SET operations for each port.\ngpioPortB: Index to select Port B.\nDOUT: Data Output register for atomic SET operation.\n\nSimilarly:\nGPIO-&gt;P_CLR[gpioPortC].DOUT = (1 &lt;&lt; 5); // Clear pin 5 on Port C\nGPIO-&gt;P_TGL[gpioPortD].DOUT = (1 &lt;&lt; 1) | (1 &lt;&lt; 3); // Toggle pins 1 and 3 on Port D\n\n\n2.7.4 Advantages of Using Structures and the -&gt; Operator\n\nCode Clarity: Clear and readable syntax for hardware register access.\nPortability: Standardized structure definitions across different microcontrollers.\nEfficiency: Direct register access through pointer dereferencing minimizes CPU cycles.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/2_programmingwithc.html#exercise-multiple-choice-questions",
    "href": "contents/core/2_programmingwithc.html#exercise-multiple-choice-questions",
    "title": "2  Programming Embedded Systems with C",
    "section": "2.8 Exercise: Multiple Choice Questions",
    "text": "2.8 Exercise: Multiple Choice Questions\n\nWhat is the primary reason C is preferred for embedded systems programming?\n\nUser-friendly syntax\nHigh-level abstraction\nLow-level hardware access and efficiency\nAutomatic memory management\n\nCorrect Answer: C\nWhich header file is commonly included in an embedded C program for fixed-width integer types?\n\nstdio.h\nstdint.h\nstring.h\nstdlib.h\n\nCorrect Answer: B\nWhat happens if a variable exceeds the maximum value of its data type?\n\nIt goes back to zero\nIt causes a system crash\nIt triggers an interrupt\nIt generates a compiler warning\n\nCorrect Answer: A\nWhich data type is best suited for a counter variable on an 8-bit microcontroller?\n\nint\nuint8_t\nfloat\ndouble\n\nCorrect Answer: B\nWhat is the key advantage of using floating-point data types sparingly in embedded systems?\n\nReduced memory usage\nIncreased processing speed\nBetter precision\nSimplified syntax\n\nCorrect Answer: A\nWhich of the following is an example of memory alignment in embedded systems?\n\nAddress divisible by 2 for a short integer\nRandomly allocated memory address\nUsing dynamic memory allocation\nOverwriting stack memory\n\nCorrect Answer: A\nWhat does the bitwise ‘AND’ operator do in GPIO manipulation?\n\nSets specific bits\nClears specific bits\nMasks specific bits\nToggles specific bits\n\nCorrect Answer: C\nWhat is the purpose of the ‘SET’ register in GPIO control?\n\nClear specific GPIO pins\nToggle specific GPIO pins\nSet specific GPIO pins\nRead GPIO pin status\n\nCorrect Answer: C\nWhich best describes data coercion in embedded systems?\n\nAutomatic type conversion\nForced memory alignment\nManual data truncation\nDynamic memory reallocation\n\nCorrect Answer: A\nWhat happens when a uint16_t variable is assigned to a uint8_t variable with a value greater than 255?\n\nValue remains unchanged\nCompiler error\nValue is truncated\nSystem crash\n\nCorrect Answer: C\nWhich of the following prevents race conditions in GPIO control?\n\nUsing ‘DOUT’ register\nUsing ‘SET’ and ‘CLR’ registers atomically\nDisabling interrupts\nUsing global variables\n\nCorrect Answer: B\nWhy is aligned memory access preferred in embedded systems?\n\nBetter energy efficiency\nIncreased memory usage\nReduced CPU latency\nDynamic memory allocation\n\nCorrect Answer: C\nWhat is the main function of bitwise shift operators (‘&lt;&lt;’ and ‘&gt;&gt;’) in embedded C?\n\nInverting bits\nMultiplying or dividing by powers of two\nClearing specific bits\nReading GPIO pin status\n\nCorrect Answer: B\nWhat should you avoid when working with GPIO atomic operations?\n\nMixing standard ‘DOUT’ and atomic operations\nUsing specific masks\nDocumenting shared resources\nTesting configurations\n\nCorrect Answer: A\nWhich fixed-width integer type ensures consistent size across platforms?\n\nint\nlong\nuint16_t\nshort\n\nCorrect Answer: C",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>**Programming Embedded Systems with C**</span>"
    ]
  },
  {
    "objectID": "contents/core/3_devkitoverview.html",
    "href": "contents/core/3_devkitoverview.html",
    "title": "3  EFR32xG24 Development Kit Overview",
    "section": "",
    "text": "3.1 Key Features of the EFR32xG24 Development Kit\nThe EFR32xG24 Development Kit (xG24-DK2601B) is a versatile platform designed for prototyping and evaluating applications using the EFR32MG24 Wireless Gecko System-on-Chip (SoC), EFR32MG24B310F1536IM48-B. It serves as an ideal platform for developing energy-efficient IoT devices, offering advanced hardware features, debugging capabilities, and seamless integration with development tools such as Simplicity Studio. It also contains a built-in AI/ML Hardware Accelerator.\nThe key components and features of this kit include:",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**EFR32xG24 Development Kit Overview**</span>"
    ]
  },
  {
    "objectID": "contents/core/3_devkitoverview.html#key-features-of-the-efr32xg24-development-kit",
    "href": "contents/core/3_devkitoverview.html#key-features-of-the-efr32xg24-development-kit",
    "title": "3  EFR32xG24 Development Kit Overview",
    "section": "",
    "text": "EFR32MG24 Wireless Gecko SoC: ARM Cortex-M33 processor operating at 78 MHz, with 1536 KB Flash and 256 KB RAM.\nConnectivity: High-performance 2.4 GHz radio for Bluetooth and other wireless protocols.\nOn-Board Sensors:\n\nSi7021 Relative Humidity and Temperature Sensor.\nSi7210 Hall Effect Sensor.\nICS-43434 MEMS Stereo Microphones.\nICM-20689 6-Axis Inertial Sensor.\nVEML6035 Ambient Light Sensor.\nBMP384 Barometric Pressure Sensor.\n\nMemory: 32 Mbit external SPI flash for Over-The-Air (OTA) firmware updates and data logging.\nPower Options: USB, coin cell battery (CR2032), or external battery.\nDebugging Tools:\n\nSEGGER J-Link On-Board Debugger.\nPacket Trace Interface (PTI).\nMini Simplicity Connector for advanced debugging.\n\nUser Interface: Two push buttons, an RGB LED, and a virtual COM port.\nConnectivity Interfaces: I2C, SPI, UART, and Qwiic Connector.\n\n\n3.1.1 Development Environment and Tools\nThe development kit is fully supported by Silicon Labs’ Simplicity Studio, an integrated development environment (IDE) offering:\n\nProject creation and device configuration.\nReal-time energy profiling and debugging tools.\nWireless network analysis with Packet Trace Interface (PTI).\nPre-built example projects and libraries for rapid prototyping.\n\n\n\n3.1.2 Power Management\nThe kit offers flexible power options, including:\n\nUSB power supply through a Micro-B connector.\nCoin cell battery (CR2032) for portable applications.\nExternal battery via a dedicated header.\nAutomatic power source switchover for seamless transitions.\n\nExample Configuration for USB Power Supply:\nPower supplied via USB Micro-B connector:\n- VBUS regulated to 3.3V for SoC and peripherals.\n- Automatic switchover when USB is connected.\n\n\n3.1.3 Debugging and Virtual COM Port\nThe built-in SEGGER J-Link debugger allows:\n\nOn-chip debugging via Serial Wire Debug (SWD) interface.\nReal-time packet trace using Packet Trace Interface (PTI).\nSerial communication using Virtual COM Port (VCOM).\n\nExample UART Configuration for VCOM:\nBaud rate: 115200 bps\nData bits: 8\nParity: None\nStop bits: 1\n\n\n3.1.4 GPIO and Peripheral Access\nThe development kit provides 20 breakout pads, exposing GPIO pins, I2C, UART, and SPI interfaces. These pads follow the EXP header pinout standard, ensuring compatibility with expansion boards. Each sensor is optimized for low power consumption.\n\n\n3.1.5 Best Practices for Overall Project Development\n\nUse Simplicity Studio for project management and debugging.\nEnable only necessary peripherals to conserve power.\nUse GPIO atomic operations for time-critical applications.\nValidate sensor connections using test scripts.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**EFR32xG24 Development Kit Overview**</span>"
    ]
  },
  {
    "objectID": "contents/core/3_devkitoverview.html#sensors-and-interfaces",
    "href": "contents/core/3_devkitoverview.html#sensors-and-interfaces",
    "title": "3  EFR32xG24 Development Kit Overview",
    "section": "3.2 Sensors and Interfaces",
    "text": "3.2 Sensors and Interfaces\nThe EFR32xG24 Development Kit integrates multiple onboard sensors interfaced through GPIO, I2C, or SPI connections, ensuring precise communication and control.\n\nSi7021 Relative Humidity and Temperature Sensor\nThe Si7021 is a high-precision digital humidity and temperature sensor featuring a factory-calibrated output and low power consumption, making it suitable for IoT and embedded applications.\nKey Features:\n\nRelative humidity accuracy: ±3%\nTemperature accuracy: ±0.4°C\nOperating voltage: 1.9V to 3.6V\nUltra-low standby current: 60 nA\n\nApplications:\n\nEnvironmental monitoring systems\nHVAC control\nSmart home automation\n\nThe sensor is connected through I2C, and its thermal isolation reduces self-heating effects, ensuring more accurate temperature readings.\n\n\nSi7210 Hall Effect Sensor\nThe Si7210 is a highly sensitive Hall effect sensor capable of detecting magnetic field changes with excellent precision. It is often used in applications requiring contactless position sensing.\nKey Features:\n\nMagnetic sensitivity: ±2.5 mT\nI2C communication interface\nProgrammable magnetic thresholds\nFactory-calibrated accuracy\n\nApplications:\n\nProximity sensing\nPosition detection\nReed switch replacement\n\nThe Si7210 offers real-time magnetic field measurements and is configured via the I2C bus.\n\n\nICS-43434 MEMS Stereo Microphones\nThe ICS-43434 microphones are omnidirectional MEMS microphones with I2S digital output. They are suitable for audio signal processing and voice recognition systems.\nKey Features:\n\nFrequency response: 50 Hz – 20 kHz\nDigital I2S output\nLow power consumption\nHigh Signal-to-Noise Ratio (SNR)\n\nApplications:\n\nVoice recognition systems\nAcoustic event detection\nEnvironmental noise monitoring\n\nThe microphones are mounted on the bottom side of the development board, with sound pathways designed for optimal acoustic performance.\n\n\nICM-20689 6-Axis Inertial Sensor\nThe ICM-20689 integrates a 3-axis gyroscope and a 3-axis accelerometer for precise motion and orientation tracking.\nKey Features:\n\n3-axis gyroscope and 3-axis accelerometer\nProgrammable digital filters\nIntegrated 16-bit ADC\nSPI interface for high-speed communication\n\nApplications:\n\nMotion detection systems\nGesture-based controls\nOrientation tracking\n\nThe sensor is positioned near the geometrical center of the board, minimizing measurement bias caused by physical placement.\n\n\nVEML6035 Ambient Light Sensor\nThe VEML6035 is a high-precision ambient light sensor that supports a digital I2C interface. It is designed for automatic brightness control and energy-saving applications.\nKey Features:\n\nWide dynamic range\nLow power consumption\nHigh accuracy\nI2C communication\n\nApplications:\n\nDisplay backlight adjustment\nSmart lighting systems\nProximity detection\n\nThe sensor is factory-calibrated for optimal accuracy and sensitivity across a wide range of light intensities.\n\n\nBMP384 Barometric Pressure Sensor\nThe BMP384 is a high-precision absolute barometric pressure sensor with an integrated temperature sensor suitable for environmental monitoring and altitude estimation.\nKey Features:\n\nPressure accuracy: ±0.5 hPa\nTemperature accuracy: ±0.5°C\nI2C/SPI communication interface\nIntegrated noise reduction filter\n\nApplications:\n\nWeather station systems\nAltitude estimation\nDrone stabilization systems\n\nThe BMP384 sensor uses an internal noise-reduction filter to improve data accuracy during high-resolution measurements.\n\n\nBest Practices for Sensor Integration\nTo ensure optimal performance when working with the onboard sensors:\n\nAlways enable sensor power through the appropriate GPIO pins before initialization.\nAvoid floating GPIO lines connected to sensors.\nValidate sensor connections and configurations using test scripts.\nMinimize concurrent access to shared I2C or SPI lines.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**EFR32xG24 Development Kit Overview**</span>"
    ]
  },
  {
    "objectID": "contents/core/4_efr32programming.html",
    "href": "contents/core/4_efr32programming.html",
    "title": "4  EFR32 I/O Programming",
    "section": "",
    "text": "4.1 EFR32XG24 GPIO Overview\nAs displayed in Figure 4.1, the EFR32XG24 microcontroller series includes multiple GPIO ports (A, B, C, and D), with each port supporting up to 16 pins. The key GPIO ports and pins that are available on the specific chip used in the EFR32XG24 Dev Kit are:\nEach GPIO pin can be individually configured for various modes, including input, output, and alternate functions.\nNote that on the EFR32XG24 Dev Kit, only some pins are broken out, that is, available for use via the expansion headers on the left and right sides of the board. These pins on the expansion header, which may be found in the EFR32XG24 Dev Kit User Guide on page 19 are displayed in Figure 4.2.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>**EFR32 I/O Programming**</span>"
    ]
  },
  {
    "objectID": "contents/core/4_efr32programming.html#efr32xg24-gpio-overview",
    "href": "contents/core/4_efr32programming.html#efr32xg24-gpio-overview",
    "title": "4  EFR32 I/O Programming",
    "section": "",
    "text": "PA00-PA09\nPB00-PB05\nPC00-PC09\nPD00-PD05\n\n\n\n\n4.1.1 Clock Management Unit (CMU)\nThe Clock Management Unit (CMU) controls the clock signals for various peripherals, including GPIO. Before using GPIO, its corresponding clock must be enabled by setting the appropriate bit in the CMU_CLKEN0 register:\nCMU-&gt;CLKEN0 |= 1 &lt;&lt; 26;\nThis ensures that the GPIO module is both powered up and ready for use with a clock connected.\nThe CMU_CLKEN0 register also allows activating the clock to other peripherals through the use of the same code as above. The only required change is the bit to modify, by changing the number of bits to shift the 1 left to one of the options shown in Figure 4.3.\n\n\n\nFigure 4.1: Pinout of QFN-48 packaged EFR32MG24 microcontroller (EFR32MG24 Datasheet page 107)\n\n\n\n\n\nFigure 4.2: EFR32XG24 Dev Kit Expansion Header Pinout (UG524 page 19)\n\n\n\n\n\nFigure 4.3: Peripherals available to enable in the CMU_CLKEN0 register (Reference manual page 173)\n\n\n\n\n\nFigure 4.4: The MODEL and MODEH registers for EFR32MG24 GPIO Port A (Reference manual page 851)\n\n\n\n\n4.1.2 GPIO Configuration\nEach GPIO pin can serve multiple functions controlled by the MODEL and MODEH registers:\n\nMODEL: Configures pins 0-7 of the port.\nMODEH: Configures pins 8-15 of the port.\n\nEach pin mode is represented by 4 bits, supporting modes such as:\n\n0: Disabled\n1: Input\n2: Input pull-up/down\n4: Push-pull (Output)\n\nIn pull-up/pull-down mode, the value of the DOUT register (covered later) determines the pull direction, with a 1 being pull-up and 0 being pull-down.\nTo set a pin mode programmatically:\nGPIO-&gt;P[gpioPortX].MODEL |= mode &lt;&lt; (4 * n);\nFor pin numbers 8-15:\nGPIO-&gt;P[gpioPortX].MODEH |= mode &lt;&lt; (4 * (n - 8));\nA total of 16 pin modes are available for each GPIO pin. While many of these modes are not used in basic applications, Figure 4.5 displays all of the available options.\n\n\n\nFigure 4.5: Mode register value options for each GPIO pin (Reference manual page 851)\n\n\n\n\n4.1.3 GPIO Output Control\nGPIO output can be managed using the following registers:\n\nDOUT: Directly outputs data to pins.\nSET: Atomically sets specified bits.\nCLR: Atomically clears specified bits.\nTGL: Atomically toggles specified bits.\n\nExample of setting and clearing pins:\nGPIO-&gt;P[gpioPortD].DOUT |= 1 &lt;&lt; 2; // Set pin 2 of Port D\nGPIO-&gt;P[gpioPortD].DOUT &= ~(1 &lt;&lt; 2); // Clear pin 2 of Port D\n\n\n4.1.4 GPIO Input Control\nGPIO pins configured as inputs can be read using the DIN register:\nuint8_t pinState = (GPIO-&gt;P[gpioPortX].DIN &gt;&gt; n) & 1;\nThis reads the state of pin n and returns either 0 or 1.\n\n\n4.1.5 Using emlib for GPIO\nEFR32 provides the emlib hardware abstraction layer (HAL) for GPIO configuration:\n\nGPIO_PinModeSet(port, pin, mode)\nGPIO_PinOutSet(port, pin)\nGPIO_PinOutClear(port, pin)\nGPIO_PinOutToggle(port, pin)\nGPIO_PinInGet(port, pin)\n\nExample of setting a pin as output using emlib:\nGPIO_PinModeSet(gpioPortD, 2, gpioModePushPull); // Set the Push Pull Mode of Pin 2 of Port D\nGPIO_PinOutSet(gpioPortD, 2); // Set pin 2 of Port D\n\n\n4.1.6 Practical Example: Blinking an LED\nA simple example of GPIO programming is blinking an LED connected to a GPIO pin:\nGPIO_PinModeSet(gpioPortD, 2, gpioModePushPull);\nwhile (1) {\n    GPIO_PinOutToggle(gpioPortD, 2);\n    delay(1000);\n}\nThis toggles the LED state every second.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>**EFR32 I/O Programming**</span>"
    ]
  },
  {
    "objectID": "contents/core/5_efr32applications.html",
    "href": "contents/core/5_efr32applications.html",
    "title": "5  Applications of EFR32 I/O",
    "section": "",
    "text": "5.1 7-Segment Displays\n7-segment displays are commonly found in user-facing embedded systems, such as clock radios, household appliances, vehicles, and industrial equipment. While LED status indicators are often used for simple devices, they cannot communicate detailed information such as sensor readings or error codes. Gaining traction in the 1970s with the advent of LED technology, 7-segment displays bridge the gap between basic indicator lights and more complex graphic screens, commonly offering one or multiple digits composed of seven LED digit segments plus a decimal point or colon.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>**Applications of EFR32 I/O**</span>"
    ]
  },
  {
    "objectID": "contents/core/5_efr32applications.html#segment-displays",
    "href": "contents/core/5_efr32applications.html#segment-displays",
    "title": "5  Applications of EFR32 I/O",
    "section": "",
    "text": "5.1.1 Segments\n7-segment displays are composed of a group of LED segments arranged in an “8” pattern, allowing every digit from 0-9 plus a limited selection of letters to be readable.\nThese segments are commonly labeled A-G in a clockwise manner, with A being the top segment and G being the middle segment. Depending on the display, the segments may be wired in a common anode (LED positive terminal) or common cathode (LED negative terminal) configuration. Depending on the configuration, a slightly different circuit with inverted code logic may be necessary.\nAdditionally, as each segment is a simple LED, current-limiting resistors are a necessary inclusion in the circuit. In some cases, it may be acceptable to place a single resistor between in series with the common pin, especially if the resistor is of a high value to significantly limit the segment’s brightness. However, in most cases, it is ideal to adhere to the best practice of placing a current-limiting resistor in series with each segment so that manufacturing discrepancies between segments do not allow any individual segment to endanger itself with a high current.\n\n\n5.1.2 Wiring\nA 7-segment display will allocate a significant number of pins on a microcontroller, often using up nearly an entire GPIO port. If the decimal point is not used, one pin may be saved, but in many cases, it is beneficial to use a BCD to 7-segment decoder IC, such as the 74LS147, or even an 8-bit serial-in, parallel-out shift register such as the 74HC595 for greater GPIO pin efficiency. However, for the purposes of this guide, the 7-segment display will be directly connected to the microcontroller, using 8 GPIO pins.\nIn an ideal design, such as when building a PCB carrier board for the EFR32MG24 chip, a bank of pins such as PC00-PC07 may be used, allowing the GPIO port C MODEL register to be written in its entirety and full bytes written to the pin set and clear ports.\nHowever, the EFR32XG24 Dev Kit board does not break out a single port in its entirety, therefore requiring the display to share pins between Port A and Port C. Segments A-E will use PC01-PC05, while F, G, and the decimal point (DP) will be connected to PA05-PA07, as displayed in Figure 5.1. This requires in code an array of GPIO ports and pins to look up the right one for a given segment:\n// use gpioPortC (2) pins 1-5 for A-E, and gpioPortA (0) pins 5-7 for F, G, and DP\n//                               A  B  C  D  E  F  G  .\nconst uint8_t segment_ports[] = {2, 2, 2, 2, 2, 0, 0, 0};\nconst uint8_t segment_pins[] =  {1, 2, 3, 4, 5, 5, 6, 7};\n\n\n\nFigure 5.1: Wiring diagram for a single common cathode 7-segment display\n\n\n\n\n5.1.3 Digit display logic\nWith these arrays created, the pattern of segments to enable for any character can now be defined. As there are eight segments in total, it makes sense to represent these patterns as bits in a byte, allowing for straightforward storage and lookup. To construct this byte, one may represent segment A as bit 0, B as bit 1, and so on until DP is bit 7. This results in Table 5.1, displaying the construction of hexadecimal codes for digits 0-9.\n\n\nLookup table for 7-segment display digit codes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigit\nbit 7\nbit 6\nbit 5\nbit 4\nbit 3\nbit 2\nbit 1\nbit 0\nHexadecimal\n\n\n\n.\nG\nF\nE\nD\nC\nB\nA\n\n\n\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0x3F\n\n\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0x06\n\n\n2\n0\n1\n0\n1\n1\n0\n1\n1\n0x5B\n\n\n3\n0\n1\n0\n0\n1\n1\n1\n1\n0x4F\n\n\n4\n0\n1\n1\n0\n0\n1\n1\n0\n0x66\n\n\n5\n0\n1\n1\n0\n1\n1\n0\n1\n0x6D\n\n\n6\n0\n1\n1\n1\n1\n1\n0\n1\n0x7D\n\n\n7\n0\n0\n0\n0\n0\n1\n1\n1\n0x07\n\n\n8\n0\n1\n1\n1\n1\n1\n1\n1\n0x7F\n\n\n9\n0\n1\n1\n0\n1\n1\n1\n1\n0x6F\n\n\n\n\nThese hexadecimal codes for each digit can then be inserted into another array, with the array index mapping a desired digit to its segment code. In a later exercise, you will be required to expand this array to support hexadecimal digits as well.\n\n\n5.1.4 Display driver code\nNow that this look-up array for digits is implemented, driving the 7-segment display is trivial. Each GPIO pin in use must be set up as an output. Each pin may then be looped through, and set or cleared depending on if its corresponding bit in the hexadecimal code is set. This can be achieved by shifting the hexadecimal code right by the loop iterator variable, then evaluating based on the bitwise AND of the shifted code and 1.\nfor (int i = 0; i &lt; 8; i++) // loop through all segments\n{ \n    if ((segments[arbitrary_digit] &gt;&gt; i) & 1) // look up hex code, shift right, AND\n    {\n        GPIO-&gt;P_SET[segment_ports[i]].DOUT = 1 &lt;&lt; segment_pins[i]; // turn on segment\n    }\n    else\n    {\n        GPIO-&gt;P_CLR[segment_ports[i]].DOUT = 1 &lt;&lt; segment_pins[i]; // turn off segment\n    }\n}\nIn this example, we use the segments array to loop up the hex code for a given arbitrary_digit, which could be an integer literal or a variable. The looked up code is shifted right based on the index of the current digit to be illuminated or turned off. With the bit of interest now moved to bit 0, it is compared with 1 to determine the appropriate state for the segment.\n\n\n5.1.5 Multiple digits\nIn many applications, multiple 7-segment digits are necessary to display a larger number or other more detailed information. Even displaying time in a 12- or 24-hour format requires four digits. Therefore, it is often beneficial to combine multiple 7-segment displays into a single module, and these are commonly available in 2, 4, 6, or 8 digit configurations. However, using 8 GPIO pins per segment can quickly waste all available microcontroller pins. Instead, all digits in a module multiplex, or share, segment pins, which means that all segments, if illuminated at the same time, would show the same character. To facilitate this multiplexing, each digit has its own separate common cathode or common anode pin, which can be connected or disconnected to power. Each digit may then be lit one after another, with only one on at a given time, and this process is constantly repeated to create the effect that all digits are constantly on.\nThis does necessitate the use of an NPN transistor for each digit to switch the common pin load of the digit on and off, as the microcontroller cannot sink this significant current into a single GPIO. An example circuit is included in Figure 5.2, demonstrating the connections of a two-digit module with a common cathode configuration to an MCU.\n\n\n5.1.6 Multiple digits logic\nThe same code may be used for driving multiple digits as a single digit; after all, the only difference is that the lit digit must be changed repeatedly. It is therefore ideal to move the single digit driver code to its own function so that it may be reused. The infinite loop must be adjusted to drive each digit’s transistor base pin high, then display a given digit, and finally switch the transistor back off before repeating the process for the next digit. This must be done quickly to avoid flicker at a frequency that is visible to the human eye, but not so quickly that the LEDs in each segment do not have time to reach their full brightness. Therefore, a few milliseconds of delay may be necessary while each digit is on before quickly switching to the next digit.\n\n\n\nFigure 5.2: Two common-cathode 7-segment display digits switches by transistors\n\n\n\n\n5.1.7 Exercise: Displaying hexadecimal numbers\nComplete Table 5.1 with the additional hexadecimal digits A-F, and expand the array. Then, try displaying 8-bit numbers in hexadecimal format with a two-digit 7-segment display module.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>**Applications of EFR32 I/O**</span>"
    ]
  },
  {
    "objectID": "contents/core/5_efr32applications.html#parallel-lcd-displays",
    "href": "contents/core/5_efr32applications.html#parallel-lcd-displays",
    "title": "5  Applications of EFR32 I/O",
    "section": "5.2 Parallel LCD displays",
    "text": "5.2 Parallel LCD displays\nWhen more letters or symbols must be displayed to a user than is practical with a simple 7-segment display, or a graphical user interface is it is common to use a screen. Early computers generated signals to drive CRT screens, with a limited number of display lines and colors. Now, while full-color, high resolution monitors and other displays are widespread, it is still common to find smaller, often monochrome screens used in embedded systems due to their simplicity and minimal power consumption. In this section, we will learn about interfacing with a liquid crystal display (LCD) screen that can display two lines of 5x8 pixel characters. These low-cost displays are commonly found on budget 3D printers, control units for machinery, or in vehicle entertainment systems.\n\n5.2.1 16x2 Character LCD\nAn inexpensive character-based LCD module often contains 2-4 rows of 8-20 characters. In this case, the common LCD1602 module with 2 rows of 16 characters will be used. At the top of the display is a row of pins for powering and controlling the display. Table 5.2 displays details for each pin, but most commonly found on these displays are power and ground for the display, a separate anode and cathode for the backlight LED, a contrast adjustment, and a number of data and control signals. To understand how to interface with the LCD, we must examine the display’s built in controller.\n\n\n5.2.2 LCD Controller\nThe LCD module has an on-board Hitachi HD44780U controller that generates signals for the individual pixels of the display. The HD44780U is based on an original 1980s design, retaining command and feature parity while supporting modern microcontroller interfaces. It has two host-facing I/O registers as well as internal memory, meaning that data written to the display remains until it is next updated, reducing host MCU processor load. A 4- and 8-bit interface allows writing to, or reading from, both the instruction register or data register, which are used for configuration and character output, respectively. Therefore, these displays are known as using a parallel interface, as multiple bits of data are transferred at the same instant. More advanced displays may also offer additional interfaces that we will learn about later, such as I2C or SPI. It is important to read and understand the datasheet for the LCD controller to learn how to interface with it. The datasheet is linked here, but excerpts will be taken from it in this section: https://www.sparkfun.com/datasheets/LCD/HD44780.pdf\n\n\n\n\n\n\n\nFigure 5.3: Pinout for commonly available 16x2 LCD modules\n\n\n\n\nTable 5.2: Pin designations and descriptions for the 16x2 LCD display\n\n\n\n\n\n\n\nPin\nSymbol\nDescription\n\n\n\n\n1\nGND\nDisplay ground.\n\n\n2\nVCC\nDisplay power. Connect to 5V.\n\n\n3\nV0\nDisplay contrast adjustment. 0-5V range.\n\n\n4\nRS\nRegister select. 0 for instructions, 1 for data.\n\n\n5\nR/W\nRead/write. 0 for write, 1 for read.\n\n\n6\nE\nEnable. Starts data read/write operation.\n\n\n7\nD0\nData bit 0, used in 8-bit mode.\n\n\n8\nD1\nData bit 1, used in 8-bit mode.\n\n\n9\nD2\nData bit 2, used in 8-bit mode.\n\n\n10\nD3\nData bit 3, used in 8-bit mode.\n\n\n11\nD4\nData bit 4, used in 4-bit and 8-bit mode.\n\n\n12\nD5\nData bit 4, used in 4-bit and 8-bit mode.\n\n\n13\nD6\nData bit 4, used in 4-bit and 8-bit mode.\n\n\n14\nD7\nData bit 4, used in 4-bit and 8-bit mode.\n\n\n15\nA\nBacklight LED anode. Connect to 5V.\n\n\n16\nK\nBacklight LED cathode. Connect to GND.\n\n\n\n\n\n\n5.2.3 LCD Wiring\nAs referenced above, these displays support both a 4-bit and an 8-bit data transfer mode, with the 8-bit data length allowing for faster and simpler transmissions while the 4-bit data length increases software complexity but requires fewer GPIO pins to be allocated.\nIn both cases, the display also requires three additional control signals, RS, RW, and E. The RS line selects between the instruction register (if set to 0), and the data register (if set to 1) of the HD44780 controller, allowing data sent to be interpreted as a command or a character to display. The RW line configures the data pins for read or write mode, from the perspective of the host MCU. Because the display will receive commands and data from the MCU most of the time, the RW line will often be 0, however, in some cases such as reading the address of the display cursor or the display’s busy signal, this line should be brought high. Finally, the E—or enable—signal causes a data transfer to occur. When writing to the display, the data and control lines should first be set up, and then the enable line quickly toggled on then back off, causing the HD44780U to accept the command or data.\nIn total, the 4-bit data mode will use a minimum of 7 GPIOs, and the 8-bit mode a minimum of 11 GPIOs. While they are not prohibitive, these are significant pin allocations for a single peripheral, and care must be taken when designing an embedded system to make good use of available pins.\nTherefore, this section will take into account the additional complexity of the 4-bit data mode, making the 8-bit mode comparatively trivial to implement. To begin, the LCD should be connected to the EFR32XG24 Dev Kit as shown in the schematic in Figure 5.4 and the wiring diagram in Figure 5.5.\n\n\n\n\n\n\n\nFigure 5.4: Schematic diagram of LCD connections to EFR32XG24 Dev Kit\n\n\n\n\n\n\n\n\n\nFigure 5.5: Wiring diagram for LCD connections to EFR32XG24 Dev Kit\n\n\n\n\n5.2.4 LCD Data Transfer\nThe LCD accepts command and data bytes on the four or eight connected data lines, on the falling edge of a single pulse of the enable line. The three control lines and all of the data lines must first be written to, so that the data on them is valid at the time of the enable line pulse. In 8-bit mode, each pulse of the enable line corresponds with a single instruction or data. However, in 4-bit mode, two consecutive writes or reads are necessary to transmit a full command. The command’s byte must be split into two nibbles—groups of four bits— and then transmitted with the most significant bits (MSBs) 7:4 first, followed by the least significant bits (LSBs) 3:0. At the completion of the second data transfer, the LCD will execute the command and, after a brief period, be ready to accept more. The following code implements the protocol described above to send instructions or data to the LCD. Note that this code assumes that the control and data pins have already been configured as outputs.\nvoid lcd_nibble_write(uint8_t data, uint8_t register_select)\n{\n    lcd_wait(); // wait until busy flag is not set (covered later in chapter)\n\n    GPIO-&gt;P_CLR[DATA_port] = DATA_mask;               // clear data bits PC04-PC01\n    GPIO-&gt;P_SET[DATA_port] = (data &lt;&lt; 1) & DATA_mask; // set data bits shifted onto the correct pins\n\n    if (register_select) // data\n        GPIO-&gt;P_SET[CTRL_port] = 1 &lt;&lt; RS_pin;\n    else // command\n        GPIO-&gt;P_CLR[CTRL_port] = 1 &lt;&lt; RS_pin;\n\n    GPIO-&gt;P_SET[CTRL_port] = 1 &lt;&lt; EN_pin; // set enable\n    sl_sleeptimer_delay_millisecond(1);\n    GPIO-&gt;P_CLR[CTRL_port] = 1 &lt;&lt; EN_pin; // unset enable\n}\nThe lcd_nibble_write function takes two arguments, the first being the data (whether it be an instruction or character) to transmit, and the second being a boolean for the register select line. First, the function checks the LCD busy flag to determine if the LCD controller is ready to accept new data. This function will be discussed later, and may be implemented or replaced with a delay. The data lines are then cleared so that any previously sent data does not interfere with the new data to be written. As the data is expected in the lower four bits of the data byte, it is shifted into the correct position on the port based on the wiring diagram. Depending on the wrapper code for this function, an additional masking of the data may be wise to avoid tampering with other GPIO pins. The register select pin is also written to match the register_select argument, and finally the enable line toggled to complete the transmission.\n\n\n5.2.5 LCD Instructions\nSending a full command or character to the LCD just requires two calls to the already-implemented lcd_nibble_write function, one for each nibble that must be transmitted. It may be beneficial to write a wrapper function that does this automatically, requiring only the full byte of data, and potentially a register select argument to complete the entire process. This would involve shifting the data argument right four bits, calling lcd_nibble_write to transmit these MSBs, then masking out the upper four bits of the data argument and again calling lcd_nibble_write.\nWith the understanding of sending full commands to the LCD, it can now be properly initialized. To do so, it is necessary to consult the HD44780U datasheet to properly form the LCD commands. An excerpt from the datasheet is included in Figure 5.6.\n\n\n\n\n\n\n\nFigure 5.6: Instruction table for HD44780 (HD44780U Datasheet page 25)\n\n\nGoing through these instructions, it is clear that the command itself is determined by the place of the leftmost set bit.\nThe first couple of instructions at the top, Clear display and Return home, do not require arguments, and therefore require no bits lower than the leftmost set bit to change their behavior.\nThe next command, eEtry mode set, determines if the LCD’s internal DDRAM address counter is increased or decreased after a character is sent. The DDRAM address corresponds with the cursor position, so it is most common for bit 1 to be set for this command. Bit 0 in the entry mode set command controls if the display should be shifted, as in, the cursor remains in the same position on the display and all other letters scroll around it when a character is sent. This mode is sometimes useful for displaying a wide line of scrolling text.\nThe Display on/off control command allows the display itself, the cursor, and the cursor blinking to be turned on or off. Setting any of the argument bits for this command turns them on. For text entry, it is common for all three bits (2:0) to be set, giving a blinking cursor for the next character. For status or sensor reading displays, only bit 2 (entire display) should be set, as the cursor would be visually distracting in this case.\nThe Cursor or display shift command manually increments or decrements the cursor position, or shifts the entire display right or left. This may be used for a backspace action or for scrolling text.\nThe Function set command is important for initialization of the display. The data length bit (4) selects between 4- and 8-bit modes, with 0 representing 4 bits and 1 representing 8 bits. The value for this bit will depend on the wiring for the LCD chosen previously. The number of lines bit (3) configures the display controller for 1 or 2 lines of text, with 0 representing 1 line, and 1 representing 2 lines. The value for this bit should be chosen based on the LCD hardware in use. Finally, the character font bit (2) chooses between a 5x8 or 5x10 character font, and should also be chosen depending on the LCD’s capabilities.\nThe Set CGRAM address and Set DDRAM address are nearly identical, differing only in the number of bits available for the address and the RAM to write to. The CGRAM may be reconfigured while in operation with custom characters, and using the Set CGRAM address command is useful to select the custom character to overwrite. The DDRAM, which stores characters themselves that have been sent to the display, is more commonly used. Because the LCD DDRAM stores 80 character bytes, and the display is split into two lines, the second line begins at byte 40 of the DDRAM. This means that writing more than 16 characters on the first line will not automatically wrap to the second for many more characters; instead, the DDRAM address must be set to decimal 40 to begin the second line. For both commands, the address is specified in the bits lower than the leftmost set bit, and should be a valid address within the memory limits of the display controller.\nThe last command in the Figure 5.6 instruction table requires the R/W bit to be set and the data lines used as inputs to the host MCU. This command allows the LCD busy flag to be read using bit 6 of the LCD controller’s response. It also allows for the host MCU to read the LCD controller’s address counter in the lower bits 5:0.\nWhen using the 4-bit data length, each of these commands must be constructed by the MCU, then split into the high-order and low-order nibbles to send sequentially to the LCD.\n\n\n5.2.6 LCD Initialization\nWith all LCD commands accounted for, the LCD may now be initialized before use. Included in Figure 5.7 is the steps necessary to initialize the LCD in 4-bit mode.\nAt power-up, every LCD character will be fully filled in, initialized, and cleared before characters can be written to it. Based on Figure 5.7, despite the LCD being automatically reset at power on, a manual reset sequence is necessary to synchronize the nibble transmissions. This reset sequence uses only lcd_nibble_write, as it is not yet ready to receive full commands. After this reset sequence is completed, the 4-bit mode, number of lines, and character size are then set in a single function set command, and further configuration commands may be used to clear the display, move the cursor, or adjust scrolling before characters are written to the LCD for the first time.\n\n\n5.2.7 LCD Usage\nNow that the LCD is initialized, characters may be written to the LCD by sending their ASCII codes, split up into 4-bit nibbles, to the LCD with the RS control line now set high. This will cause the LCD to write these characters into the DDRAM, where they are directly displayed.\nMany effects may be created by combining the Set DDRAM address and cursor/display shift commands, including left, center, and right-aligned text, scrolling text, or even a small table of sensor readings.\n\n\n\nFigure 5.7: Block diagram of initialization sequence for HD44780U in 4-bit mode (HD44780U Datasheet page 46)\n\n\n\n\n5.2.8 LCD Wait\nThe LCD’s busy flag can be read while it processes commands internally. To handle this, you can implement the lcd_wait function, which repeatedly reads the busy flag by setting R/W to 0 and configuring the data lines as input. When the function should wait in a while loop reading the busy flag until the LCD is again ready to accept commands. Calling lcd_wait should be done before sending any instructions or data to the LCD, to ensure that it is ready to receive data. Alternatively, you can use the sl_sleeptimer_delay_ms function to wait for a duration longer than any command processing requires. While this latter approach is simpler, it is less effective for high-frequency display updates due to the inherent required delay. For this technique, waiting for 2 milliseconds following any command is practical and easy to implement.\n\n\n5.2.9 Exercise: Displaying a centered string\nFor this exercise, write a function that writes a c-string argument centered on the first line of the display. Check that the string passed to the function is no longer than 16 characters. With this condition met, calculate based on the length of the string the DDRAM address offset necessary to center the string.\nFor example, the string EE260 is \\(5\\) characters long. The number of characters that should be blank on the line is \\(16-5=11\\). To left align the text, the necessary offset is obviously \\(0\\). To right align the text, the offset should be \\(11\\), so that the \\(5\\) additional characters are placed directly touching the right side of the display. To center align the text, the remaining blank characters must be divided by 2. An integer division of \\(11/2=5\\) as the decimal is truncated, meaning that the necessary DDRAM offset is \\(5\\), which will leave \\(6\\) characters to the right of the text.\nThe necessary DDRAM offset may then be set using the appropriate LCD command, then the characters of the string transferred to the display.\nAs an extension to this exercise, you may write a function that takes an additional argument to select the type of alignment and display line to place an arbitrary string of text on.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>**Applications of EFR32 I/O**</span>"
    ]
  },
  {
    "objectID": "contents/core/5_efr32applications.html#keypad",
    "href": "contents/core/5_efr32applications.html#keypad",
    "title": "5  Applications of EFR32 I/O",
    "section": "5.3 Keypad",
    "text": "5.3 Keypad\nMany embedded systems with user interfaces are controlled by simple inputs, such as a joystick, multifunctional knobs, or often, a group of buttons. In cases where many buttons are required, such as for numerical or even text input, connecting a single button to its own GPIO pin is inefficient. With a 4x4 grid of buttons requiring 16 pins, an I/O expander or separate microcontroller dedicated to I/O would likely be necessary. However, a clever arrangement of switches in a grid such as this allows for pins to be multiplexed, requiring a minimum of \\(\\sqrt{\\text{\\# of buttons}}\\) pins to read each button. This works by connecting one side of each switch to a common row, and the other side of the switch to a common column. For a 4x4 grid, only 8 pins are necessary to read the entire matrix layout.\n\n5.3.1 Keypad Matrix Wiring\nCommonly available matrix keypads simply implement the row and column switch connections to pushbuttons integrated in the module. Their pinout is often just a connection for each row and column, allowing them to easily be connected to GPIO pins on an MCU, as shown in Figure 5.8.\nThe internal connections of the keypad may be displayed differently depending on preferences for the schematic. However, it is common to see the switches aligned on a 45angle, bridging between their respective row and column common lines, as illustrated in Figure 5.9.\n\n\n\n\n\n\n\nFigure 5.8: Pinout for an off-the-shelf keypad\n\n\n\n\n\n\n\n\n\nFigure 5.9: Schematic diagram of a 4x4 pushbutton switch matrix\n\n\n\n\n5.3.2 Reading Keypad Matrix\nTo read a matrix of switches, one axis should be connected to GPIO outputs. For example, we will connect the rows to output pins, writing all pins high to begin with. The other axis should be connected to internally pulled-down inputs, meaning that when any key is pressed, one of the high rows will be connected to the input, bringing it high. When this is detected, either with polling or an interrupt-based system, the microcontroller may now identify which key has been pressed.\n\n\n5.3.3 Identifying Pressed Key\nOnce the MCU has been alerted that any key has been pressed, it may now scan the switch matrix to determine the specific key. To do this, all rows should be written low, except for the first row. This may be achieved in practice by writing all rows low first, then immediately writing the first row high. The MCU may then check if any key in the first row has been pressed by again reading all of the column inputs. If any of the column inputs are high, the currently checked row and column that is high correspond with the pressed key. Otherwise, the MCU must repeat the process, writing the next row low. In this way, the pressed key can quickly be determined, and other actions can be taken based on it. This process is outlined in Figure 5.10, a flowchart showing the logic necessary for efficient detection of keypresses.\n\n\n\n\n\n\n\nFigure 5.10: Flowchart for detecting any keypress, then identifying the speci ic key\n\n\nA simplified sample implementation of this process is included below. The code implements nested-loop logic to scan a 4x4 matrix keypad using EFR32XG24 Dev Kit GPIO pins. First, the GPIO pins are initialized, with the column input pins (PC04–PC01) are configured as inputs, and the row output pins (PC05, PA07–PA05) are set to push-pull (output) mode. In the main loop, all the row pins are activated by setting them high. Then, if any of the column pins detects a high signal, representing a key press, the program iterates through each row to isolate the pressed key. During this process, all rows are brought low save for the current row of interest, and the program checks each column pin to identify the specific key pressed.\n// column inputs are on PC04-PC01\nconst uint8_t row_ports[] = {2, 0, 0, 0}; // PC05, PA07-PA05 are row outputs\nconst uint8_t row_pins[] = {5, 7, 6, 5};  // PC05, PA07-PA05 are row outputs\n\nint main(void)\n{\n    GPIO-&gt;P[gpioPortC].MODEL |= 0x1111 &lt;&lt; (1 * 4); // input mode\n\n    for (int i = 0; i &lt; 4; i++)\n        GPIO-&gt;P[row_ports[i]].MODEL |= 0x4 &lt;&lt; (row_pins[i] * 4); // output mode\n\n    while (1)\n    {\n        for (int i = 0; i &lt; 4; i++)\n            GPIO-&gt;P_SET[row_ports[i]] = 1 &lt;&lt; row_pins[i]; // set row pins\n\n        if (GPIO-&gt;P[gpioPortC].DIN & 0xF &lt;&lt; 1) // check if any key is pressed\n        {\n            for (int i = 0; i &lt; 4; i++) // loop through all columns\n            {\n                for (int j = 0; j &lt; 4; j++)\n                    GPIO-&gt;P_CLR[row_ports[j]] = 1 &lt;&lt; row_pins[j]; // clear all row pins\n                GPIO-&gt;P_SET[row_ports[i]] = 1 &lt;&lt; row_pins[i];     // set current row pin\n\n                for (int i = 0; i &lt; 4; i++)\n                {\n                    if (GPIO-&gt;P[gpioPortC].DIN &lt;&lt; 1 & 1 &lt;&lt; i) // check if column pin is high\n                    {\n                        // this is the key pressed\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n5.3.4 Power Efficiency\nA key benefit of reading a switch matrix using this technique, especially with the EFR32MG24, which supports GPIO interrupts from all Energy Management levels. This means that the processor may go into its deepest sleep state while still waiting for keypresses from the matrix. This requires interrupt logic, which will be discussed later, but the general implementation is as follows:\nThe GPIO pins for the rows may be set high, and configured to retain their values while in sleep mode. An interrupt to wake the processor from sleep may be enabled on all input pins, meaning that any keypresses will now trigger the interrupt, waking the MCU from sleep. It can now progress directly into the key identification phase, finding the pressed key and performing an action before returning to low-power sleep.\n\n\n5.3.5 Limitations of Switch Matrix\nThere exist a few limitations with this naive technique of reducing GPIO pin usage for a switch matrix, the most significant being the lack of key rollover. This means that the MCU cannot identify multiple keys being pressed, at least not with certainty.\nFinally, if the keypress time is very short, a key pressed and caught by the detection routine may already be released and missed by the identification routine. This can be compounded by switch bouncing because many keyboard matrices lack hardware debouncing, while software debouncing requires keys to be pressed for a longer period of time before the press is registered.\n\n\n5.3.6 Exercise: Propose a solution to the key rollover problem\nModern computer keyboards can detect any of their keys being pressed, as well as any combination of keys being pressed. To do this, they do not even require multiple I/O expanders or additional microcontrollers. Instead, there is electronic hardware integrated into the matrix circuit in series with every switch that ensures the proper key is detected.\nWhat could this hardware be? Explain how it may be used to support multiple simultaneous keypresses.",
    "crumbs": [
      "Embedded Systems Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>**Applications of EFR32 I/O**</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html",
    "href": "contents/core/10_ml_foundations.html",
    "title": "6  The Art and Science of Machine Learning",
    "section": "",
    "text": "6.1 Origins and Evolution\nLearning lies at the heart of intelligence, whether natural or artificial. In this chapter, we will embark on a fascinating exploration of the fundamental principles that enable machines to learn from experience. Together, we will examine both the theoretical foundations that provide a rigorous mathematical basis for machine learning, as well as the practical considerations that shape the design and implementation of modern learning systems. Our journey will take us from the historical roots of the field through to the cutting-edge research defining the current state of the art and the open challenges guiding future directions. By the end of this chapter, you will have built a comprehensive understanding of how machines can acquire, represent, and apply knowledge to solve complex problems and enhance decision-making across a wide range of domains.\nTo make our exploration as engaging and accessible as possible, I will aim to break down complex ideas into more easily digestible parts, building up gradually to the more advanced concepts. Along the way, I will make use of intuitive analogies, illustrative examples, and step-by-step explanations to help illuminate key points. Please feel free to ask questions or share your own insights at any point - learning is an interactive process and your contributions will only enrich our discussion!\nWith that in mind, let’s begin our journey into the art and science of machine learning.\nTo fully appreciate the current state and future potential of machine learning, it is helpful to understand its historical context and developmental trajectory. In this section, we will trace the origins of the field and highlight the pivotal advances that have shaped its evolution.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html#origins-and-evolution",
    "href": "contents/core/10_ml_foundations.html#origins-and-evolution",
    "title": "6  The Art and Science of Machine Learning",
    "section": "",
    "text": "6.1.1 Historical Context\nThe dream of creating intelligent machines that can learn and adapt has captivated the human imagination for centuries. In mythology and folklore around the world, we find stories of animated beings imbued with ‘artificial’ intelligence, from the golems of Jewish legends to the mechanical servants of ancient China. These ageless visions speak to a deep fascination with the idea of breathing life and cognizance into inanimate matter.\nHowever, the emergence of machine learning as a scientific discipline is a more recent development, tracing its origins to the mid-20th century. In a profound sense, the birth of machine learning as we know it today arose from the convergence of several key intellectual traditions:\nArtificial intelligence - The quest to create machines capable of intelligent behavior\nStatistics and probability theory - The mathematical tools for quantifying and reasoning about uncertainty\nOptimization and control theory - The principles for automated decision-making and goal-directed behavior\nNeuroscience and cognitive psychology - The scientific study of natural learning in biological systems\nEach of these tributaries contributed essential ideas and techniques that merged together to form the foundations of modern machine learning.\nSome key milestones in the early history of the field:\n\n1943 - Warren McCulloch and Walter Pitts publish “A Logical Calculus of the Ideas Immanent in Nervous Activity”, laying the groundwork for artificial neural networks\n1950 - Alan Turing proposes the “Turing Test” in his seminal paper “Computing Machinery and Intelligence”, providing an operational definition of machine intelligence\n1952 - Arthur Samuel writes the first computer learning program, which learned to play checkers better than its creator\n1957 - Frank Rosenblatt invents the Perceptron, an early prototype of artificial neural networks capable of learning to classify visual patterns\n1967 - Covering numbers and the Vapnik–Chervonenkis dimension (VC dimension) introduced in the groundbreaking work of Vladimir Vapnik and Alexey Chervonenkis, providing the foundations for statistical learning theory\n\nThese pioneering efforts laid the conceptual and technical groundwork for the subsequent decades of research that grew the field into the thriving discipline it is today.\n\n\n6.1.2 From Rule-Based to Learning Systems\nIn its early stages, artificial intelligence research focused heavily on symbolic logic and deductive reasoning. The prevailing paradigm was that of “expert systems” - computer programs that encoded human knowledge and expertise in the form of explicit logical rules. A canonical example was MYCIN, a program developed at Stanford University in the early 1970s to assist doctors in diagnosing and treating blood infections. MYCIN’s knowledge base contained hundreds of IF-THEN rules obtained by interviewing expert physicians, such as:\nIF (organism-1 is gram-positive) AND \n   (morphology of organism-1 is coccus) AND\n   (growth-conformation of organism-1 is chains)\nTHEN there is suggestive evidence (0.7) that\n     the identity of organism-1 is streptococcus\nBy chaining together inferences based on these rules, MYCIN could arrive at diagnostic conclusions and treatment recommendations that rivaled those of human specialists in its domain.\nHowever, the handcrafted knowledge-engineering approach of early expert systems soon ran into serious limitations:\n\nKnowledge acquisition bottleneck: Extracting and codifying expert knowledge proved to be extremely time-consuming and prone to inconsistencies and biases.\nBrittleness and inflexibility: Rule-based systems struggled to handle noisy data, adapt to novel situations, or keep up with changing knowledge.\nOpaque “black box” reasoning: The complex chains of inference generated by expert systems were often difficult for humans to inspect, understand, and debug.\nInability to learn from experience: Once programmed, rule-based systems remained static and could not automatically improve their performance or acquire new knowledge.\n\nThese shortcomings highlighted the need for a fundamentally different approach - one that could overcome the rigidity and opacity of handcrafted symbolical rules and instead acquire knowledge directly from data.\n\n\n6.1.3 The Statistical Revolution\nThe critical shift from rule-based to learning systems was catalyzed by two key insights: Many real-world domains are intrinsically uncertain and subject to noise, necessitating a probabilistic treatment. Expertise is often implicit and intuitive rather than explicit and axiomatic, making it more amenable to statistical extraction than symbolic codification.\nConsider again the task of medical diagnosis that systems like MYCIN sought to automate. While it is possible to elicit a set of logical rules from a human expert, there are several complicating factors:\n\nPatients present with constellations of symptoms that are imperfectly correlated with underlying disorders.\nDiagnostic tests yield results with varying levels of accuracy and associated error rates.\nDiseases evolve over time, manifesting differently at different stages.\nTreatments have uncertain effects that depend on individual patient characteristics.\nNew diseases emerge and existing ones change in their prevalence and manifestation over time. In such an environment, definitive logical rules are the exception rather than the norm. Instead, diagnosis is fundamentally a process of probabilistic reasoning under uncertainty, based on a combination of empirical observations and prior knowledge.\n\nThe key innovation that unlocked machine learning was to reframe the challenge in statistical terms:\n\nInstead of trying to manually encode deterministic rules, the goal became to automatically infer probabilistic relationships from observational data.\nRather than requiring knowledge to be explicitly enumerated, learning algorithms aimed to implicitly extract latent patterns and regularities.\nIn place of brittle logical chains, models learned robust statistical associations that could gracefully handle noise and uncertainty.\n\nThis shift in perspective opened up a powerful new toolbox of techniques at the intersection of probability theory and optimization. Some key formal developments:\n\nMaximum likelihood estimation (Ronald Fisher, 1920s): A principled framework for inferring the parameters of statistical models from observed data.\nThe perceptron (Frank Rosenblatt, 1957): A simple type of artificial neural network capable of learning to classify linearly separable patterns.\nStochastic gradient descent (Herbert Robbins & Sutton Monro, 1951): An efficient optimization procedure well-suited to large-scale machine learning problems.\nBackpropagation (multiple independent discoveries, 1970s-1980s): An algorithm for training multi-layer neural networks by propagating errors backwards through the network.\nThe VC dimension (Vladimir Vapnik & Alexey Chervonenkis, 1960s-1970s): A measure of the capacity of a hypothesis space that quantifies the conditions for stable learning from finite data.\nMaximum margin classifiers and support vector machines (Vladimir Vapnik et al., 1990s): Powerful discriminative learning algorithms with strong theoretical guarantees.\n\nTogether, innovations like these provided the foundations for statistical learning systems that could effectively extract knowledge from raw data. They set the stage for the following decades of progress that would see machine learning mature into one of the most transformative technologies of our time.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html#understanding-learning-systems",
    "href": "contents/core/10_ml_foundations.html#understanding-learning-systems",
    "title": "6  The Art and Science of Machine Learning",
    "section": "6.2 Understanding Learning Systems",
    "text": "6.2 Understanding Learning Systems\nHaving reviewed the historical context and key conceptual shifts behind the emergence of machine learning, we are now in a position to examine learning systems in greater depth. In this section, we will explore the fundamental principles that define the learning paradigm, the central role played by data, and the nature of the patterns that learning uncovers.\n\n6.2.1 The Learning Paradigm\nAt its core, machine learning represents a radical departure from traditional programming approaches. To appreciate this, it is helpful to consider how we might go about solving a complex task such as object recognition using classical programming:\nFirst, we would need to sit down and think hard about all the steps involved in identifying objects in images. We might come up with rules like:\n\n“an eye has a roughly circular shape”\n“a nose is usually located below the eyes and above the mouth”\n“a face is an arrangement of eyes, nose and mouth”, etc.\n\nNext, we would translate these insights into specific programmatic instructions:\n\n“scan the image for circular regions”\n“check if there are two such regions in close horizontal proximity”\n“label these candidate eye regions”, etc.\n\nWe would then need to painstakingly debug and refine our program to handle all the edge cases and sources of variability we failed to consider initially.\nIf our program needs to recognize additional object categories, we would have to return to step 1 and repeat the whole arduous process for each new class.\nThe classical approach places the entire explanatory burden on the human programmer - we start from a blank slate and must explicitly spell out every minute decision and edge case handling routine.\nIn contrast, the machine learning approach follows a very different recipe:\nFirst, we collect a large dataset of labeled examples (e.g. images paired with the names of the objects they contain). We select a general-purpose model family that we believe has the capacity to capture the relevant patterns (e.g. deep convolutional neural networks for visual recognition). We specify a measure of success (e.g. what fraction of the images are labeled correctly) - this is our objective function.\nWe feed the dataset to a learning algorithm that automatically adjusts the parameters of the model so as to optimize the objective function on the provided examples. We evaluate the trained model on a separate test set to assess its ability to generalize to new cases.\n\n\n\n\n\nNotice how the emphasis has shifted:\n\nRather than having to explain “how” to solve the task, we provide examples of “what” we want and let the learning algorithm figure out the “how” for us.\nInstead of handcrafting detailed solution steps, we select a flexible model and offload the burden of tuning its parameters to an optimization procedure.\nIn place of open-ended debugging, we can run controlled experiments to objectively measure generalization to unseen data.\n\nFor a more concrete illustration, consider how we might apply machine learning to the task of spam email classification:\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# 1. Collect labeled data \nemails, labels = load_email_data()\n\n# 2. Select a model family\nvectorizer = CountVectorizer()  # convert email text to word counts\nclassifier = MultinomialNB()    # naive Bayes with multinomial likelihood\n\n# 3. Specify an objective function\ndef objective(model, X, y):  \n    return accuracy_score(y, model.predict(X))\n    \n# 4. Feed data to a learning algorithm\n# learn a spam classifier from 70% of the data \ntrain_emails, test_emails, train_labels, test_labels = train_test_split(\n        emails, labels, train_size=0.7, stratify=labels)  \nX_train = vectorizer.fit_transform(train_emails)\nclassifier.fit(X_train, train_labels)\n\n# 5. Evaluate generalization on held-out test set  \nX_test = vectorizer.transform(test_emails)\nprint(\"Test accuracy:\", objective(classifier, X_test, test_labels))\nThis simple example illustrates the key ingredients:\n\nData: A collection of example emails, along with human-provided labels indicating whether each one is spam or not.\nModel: The naive Bayes classifier, which specifies the general form of the relationship between the input features (word counts) and output labels (spam or not spam) in terms of probabilistic assumptions.\nObjective: The accuracy metric, which quantifies the quality of predictions made by the model in terms of the fraction of emails that are labeled correctly.\nLearning algorithm: The fit method of the classifier, which takes in the training data and finds the model parameters that maximize the likelihood of the observed labels.\nGeneralization: The trained model is evaluated not on the data it was trained on, but rather on a separate test set that was held out during training. This provides an unbiased estimate of how well the model generalizes to new, unseen examples.\n\nOf course, this is just a toy example intended to illustrate the basic flow. Real-world applications involve much larger datasets, more complex models, and more challenging prediction tasks. But the fundamental paradigm remains the same: by optimizing an objective function on a sample of training data, learning algorithms can automatically extract useful patterns and knowledge that generalize to novel situations.\n\n\n6.2.2 Learning from Data\nAs the spam classification example makes clear, data plays a first-class role in machine learning. Indeed, one of the defining characteristics of the field is its focus on automatically extracting knowledge from empirical observations, rather than relying solely on human-encoded expertise. In this section, we take a closer look at how learning systems leverage data to acquire and refine their knowledge.\nTo begin, it is useful to clarify what we mean by “data” in a machine learning context. At the most basic level, a dataset is a collection of examples, where each example (also known as a “sample” or “instance”) provides a concrete instantiation of the task or phenomenon we wish to learn about. In the spam classification scenario, for instance, each example corresponds to an actual email message, along with a label indicating whether it is spam or not.\nMore formally, we can think of an example as a pair (x,y), where:\n\n\\(x\\) is a vector of input features that provide a quantitative representation of the relevant properties of the example. In the case of emails, the features might be counts of various words appearing in the message.\n\\(y\\) is the target output variable that we would like to predict given the input features. For spam classification, \\(y\\) is a binary label, but in general it could be a continuous value (regression), a multi-class label (classification), or a more complex structure like a sequence or image.\n\nA dataset, then, is a collection of n such examples:\n\\[D = {(x₁, y₁), ..., (xₙ, yₙ)}\\]\nThe goal of learning is to use the dataset \\(D\\) to infer a function \\(f\\) that maps from inputs to outputs:\n\\(f: X → Y\\) such that \\(f(x) ≈ y\\) for future examples \\((x,y)\\) that were not seen during training.\nWith this formalism in mind, we can identify several key properties of data that are crucial for effective learning:\n\nRepresentativeness: To generalize well, the examples in \\(D\\) should be representative of the distribution of inputs that will be encountered in the real world. If the training data is systematically biased or skewed relative to the actual test distribution, the learned model may fail to perform well on new cases.\nQuantity: In general, more data is better for learning, as it provides a richer sampling of the underlying phenomena and helps the model to avoid overfitting to accidental regularities. The amount of data needed to achieve a desired level of performance depends on the complexity of the task and the expressiveness of the model class.\nQuality: The utility of data for learning can be undermined by issues like noise, outliers, and missing values. Careful data preprocessing, cleaning, and augmentation are often necessary to ensure that the model is able to extract meaningful signal.\nDiversity: For learning to succeed, the training data must contain sufficient variability along the dimensions that are relevant for the task at hand. If all the examples are highly similar, the model may fail to capture the full range of behaviors needed for robust generalization.\nLabeling: In supervised learning tasks, the quality and consistency of the output labels is critical. Noisy, ambiguous, or inconsistent labels can severely degrade the quality of the learned model.\n\nTo make these ideas more concrete, let’s return to the spam classification example. Consider the following toy dataset:\ntrain_emails = [\n    \"Subject: You won't believe this amazing offer!\",\n    \"Subject: Request for project meeting\",\n    \"Subject: URGENT: Update your information now!\",\n    \"Hey there, just wanted to follow up on our conversation...\",\n    \"Subject: You've been selected for a special promotion!\",\n]\n\ntrain_labels = [\"spam\", \"not spam\", \"spam\", \"not spam\", \"spam\"]\nEven without running any learning algorithms, we can identify some potential issues with this dataset:\n\nSmall quantity: Only 5 examples is not enough to learn a robust spam classifier that covers the diversity of real-world emails. With so few examples, the model is likely to overfit to idiosyncratic patterns like the specific subject lines and fail to generalize well.\nLack of diversity: The examples cover a very narrow range of email types (mainly short subject lines). A more representative sample would include a mix of subject lines, body text, sender information, etc. that better reflect the variability of real emails.\nLabel inconsistency: On closer inspection, we might question whether the labeling is fully consistent. For instance, the 4th email seems potentially ambiguous - without more context about the content of the “conversation” it refers to, it’s unclear whether it should be classified as spam or not. Inconsistent labeling is a common source of problems in supervised learning.\n\nTo address these issues, we would want to collect a much larger and more diverse set of labeled examples. We might also need to do more careful data cleaning and preprocessing, for instance:\n\nTokenizing the email text into individual words or n-grams\nRemoving stop words, punctuation, and other low-information content\nStemming or lemmatizing words to collapse related variants\nNormalizing features like word counts to avoid undue influence of message length\nChecking for and resolving inconsistencies or ambiguities in label assignments\n\nIn general, high-quality data is essential for successful learning. While it’s tempting to focus mainly on the choice of model class and learning algorithm, in practice the quality of the results is often determined by the quality of the data preparation pipeline.\nAs the saying goes, ”garbage in, garbage out”* - if the input data is full of noise, bias, and inconsistencies, no amount of algorithmic sophistication can extract meaningful patterns.*\n\n\n6.2.3 The Nature of Patterns\nHaving looked at the role of data in learning, let’s now turn our attention to the other central ingredient - the patterns that learning algorithms aim to extract. What exactly do we mean by “patterns” in the context of machine learning, and how do learning systems represent and leverage them?\nIn the most general sense, a pattern is any regularity or structure that exists in the data and captures some useful information for the task at hand. For instance, in spam classification, some relevant patterns might include:\n\nCertain words or phrases that are more common in spam messages than in normal emails (e.g. “special offer”, “free trial”, “no credit check”, etc.)\nUnusual formatting or stylistic choices that are suggestive of marketing content (e.g. excessive use of capitalization, colorful text, or images)\nSuspicious sender information, like mismatches between the stated identity and email address, or sending from known spam domains\n\nA key insight of machine learning is that such patterns can be represented and manipulated mathematically, as operations in some formal space. For instance, the presence or absence of specific words can be encoded as a binary vector, with each dimension corresponding to a word in the vocabulary:\nvocabulary = [\"credit\", \"offer\", \"special\", \"trial\", \"won't\", \"believe\", ...]\n\ndef email_to_vector(email):\n    vector = [0] * len(vocabulary)\n    for word in email.split():\n        if word in vocabulary:\n            index = vocabulary.index(word)\n            vector[index] = 1\n    return vector\n    \n# Example usage\nmessage1 = \"Subject: You won't believe this amazing offer!\"\nmessage2 = \"Subject: Request for project meeting\"\n\nprint(email_to_vector(message1))\n# Output: [0, 1, 0, 0, 1, 1, ...]\n\nprint(email_to_vector(message2))  \n# Output: [0, 0, 0, 0, 0, 0, ...]\nIn this simple “bag of words” representation, each email is transformed into a vector that indicates which words from a predefined vocabulary are present in it. Already, some potentially useful patterns start to emerge - notice how the spam message gets mapped to a vector with more non-zero entries, suggesting the presence of marketing language.\nOf course, this is a very crude representation that discards a lot of potentially relevant information (word order, punctuation, contextualized meanings, etc.). More sophisticated approaches attempt to preserve additional structure, for instance:\n\nUsing counts or tf-idf weights instead of binary indicators to capture word frequencies\nExtracting \\(n\\)-grams (contiguous sequences of \\(n\\) words) to partially preserve local word order\nApplying techniques like latent semantic analysis or topic modeling to identify thematic structures\nLearning dense vector embeddings that map words and documents to points in a continuous semantic space\n\nWhat these approaches all have in common is that they define a systematic mapping from the raw data (e.g. natural language text) to some mathematically tractable representation (e.g. vectors in a high-dimensional space). This mapping is where the “learning” in “machine learning” really takes place - by discovering the specific parameters of the mapping that lead to effective performance on the training examples, the learning algorithm implicitly identifies patterns that are useful for the task at hand.\nTo make this more concrete, let’s take a closer look at how a typical supervised learning algorithm actually goes about extracting patterns from data. Recall that the goal is to learn a function \\(f: X → Y\\) that maps from input features to output labels, such that \\(f(x) ≈ y\\) for examples \\((x,y)\\) drawn from some underlying distribution.\nIn practice, most learning algorithms work by defining a parametrized function family \\(F_θ\\) and searching for the parameter values \\(θ\\) that minimize the empirical risk (i.e. the average loss) on the training examples:\n\\[θ* = argmin_θ 1/n ∑ᵢ L(F_θ(xᵢ), yᵢ)\\]\nHere \\(L\\) is a loss function that quantifies the discrepancy between the predicted labels \\(F_θ(xᵢ)\\) and the true labels \\(yᵢ\\), and the summation ranges over the \\(n\\) examples in the training dataset.\nDifferent learning algorithms are characterized by the specific function families \\(F\\) and loss functions \\(L\\) that they employ, as well as the optimization procedure used to search for \\(θ*\\). But at a high level, they all aim to find patterns - as captured by the parameters \\(θ\\) - that enable the predictions \\(F_θ(x)\\) to closely match the actual labels \\(y\\) across the training examples.\nLet’s make this more vivid by returning to the spam classification example. A very simple model family for this task is logistic regression, which learns a linear function of the input features:\n\\[F_θ(x) = σ(θ^T x)\\]\nHere \\(x\\) is the vector of word-presence features, \\(θ\\) is a vector of real-valued weights, and \\(σ\\) is the logistic sigmoid function that “squashes” the linear combination \\(θ^T x\\) to a value between 0 and 1 interpretable as the probability that the email is spam.\nCoupled with the binary cross-entropy loss, the learning objective becomes:\n\\[θ* = argmin_θ 1/n ∑ᵢ [- yᵢ log(F_θ(xᵢ)) - (1 - yᵢ) log(1 - F_θ(xᵢ))]\\]\nwhere \\(yᵢ ∈ {0,1}\\) indicates the true label (spam or not spam) for the \\(ith\\) training example.\nSolving this optimization problem via a technique like gradient descent will yield a weight vector \\(θ*\\) such that:\n\nWeights for words that are more common in spam messages (like “offer” or “free”) will tend to be positive, increasing the predicted probability of spam when those words are present.\nWeights for words that are more common in normal messages (like “meeting” or “project”) will tend to be negative, decreasing the predicted probability of spam when those words are present.\nThe magnitude of each weight corresponds to how predictive the associated word is of spam vs. non-spam - larger positive weights indicate stronger spam signals, while larger negative weights indicate stronger non-spam signals.\n\nIn this way, the learning process automatically discovers the specific patterns of word usage that are most informative for distinguishing spam from non-spam, as summarized in the weights \\(θ*\\). Furthermore, the learned weights implicitly define a decision boundary in the high-dimensional feature space - emails that fall on one side of this boundary (as determined by the sign of \\(θ^T x\\)) are classified as spam, while those on the other side are classified as non-spam.\nThis simple example illustrates several key properties that are common to many learning algorithms:\nThe parameters \\(θ\\) provide a compact summary of the patterns in the data that are relevant for the task at hand. In this case, they capture the correlations between the presence of certain words and the spam/non-spam label.\nThe learning process is data-driven - the specific values of the weights are determined by the empirical distribution of word frequencies in the training examples, not by any a priori assumptions or hand-coded rules.\nThe learned patterns are task-specific - the weights are tuned to optimize performance on the particular problem of spam classification, and may not be meaningful or useful for other tasks.\nThe expressiveness of the learned patterns is limited by the model family - in this case, the assumption of a simple linear relationship between word presence and spam probability. More complex model families (like deep neural networks) can capture richer, more nuanced patterns.\nOf course, this is just a toy example intended to illustrate the basic principles. In practice, modern learning systems often employ much higher-dimensional feature spaces, more elaborate model families, and more sophisticated optimization procedures. But the fundamental idea remains the same - by adjusting the parameters of a flexible model to minimize the empirical risk on a training dataset, learning algorithms can automatically discover patterns that generalize to improve performance on novel examples.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html#the-nature-of-machine-learning",
    "href": "contents/core/10_ml_foundations.html#the-nature-of-machine-learning",
    "title": "6  The Art and Science of Machine Learning",
    "section": "6.3 The Nature of Machine Learning",
    "text": "6.3 The Nature of Machine Learning\nHaving examined the fundamental components of learning systems - the data they learn from and the patterns they aim to extract - we now turn to some higher-level questions about the nature of learning itself. What does it mean for a machine to “learn” in the first place? How does this process differ from other approaches to artificial intelligence? And what challenges and opportunities does the learning paradigm present?\n\n6.3.1 Learning as Induction\nAt a fundamental level, machine learning can be understood as a form of inductive inference - the process of drawing general conclusions from specific examples. In philosophical terms, this contrasts with deductive inference, which derives specific conclusions from general premises.\nConsider a classic example of deductive reasoning:\n\nAll men are mortal. (premise)\nSocrates is a man. (premise)\nTherefore, Socrates is mortal. (conclusion)\n\nHere, the conclusion follows necessarily from the premises - if we accept that all men are mortal and that Socrates is a man, we must also accept that Socrates is mortal. The conclusion is guaranteed to be true if the premises are true.\nInductive reasoning, on the other hand, goes in the opposite direction:\n\nSocrates is a man and is mortal.\nPlato is a man and is mortal.\nAristotle is a man and is mortal.\nTherefore, all men are mortal.\n\nHere, the conclusion is not guaranteed to be true, even if all the premises are true - we can never be certain that the next man we encounter will be mortal, no matter how many examples of mortal men we have seen. At best, the conclusion is probable, with a degree of confidence that depends on the number and diversity of examples observed.\nMachine learning can be seen as a form of algorithmic induction - instead of a human observer drawing conclusions from examples, we have a learning algorithm that discovers patterns in data and uses them to make predictions about novel cases. Just as with human induction, the conclusions of a machine learning model are never guaranteed to be true, but can be highly probable if the training data is sufficiently representative and the model family is appropriate for the task.\nTo make this more concrete, let’s return to the spam classification example. Recall that our goal is to learn a function \\(f\\) that maps from email features \\(x\\) to spam labels \\(y\\), such that \\(f(x) ≈ y\\) for new examples \\((x,y)\\) drawn from the same distribution as the training data.\nIn the logistic regression model we considered earlier, \\(f\\) takes the form:\n\\[f(x) = σ(θ^T x)\\]\nwhere \\(θ\\) is a vector of learned weights and \\(σ\\) is the logistic sigmoid function.\nNow, imagine that we train this model on a dataset of 1000 labeled emails, using gradient descent to find the weights \\(θ*\\) that minimize the average cross-entropy loss on the training examples. We can then apply the learned function f* to classify new emails as spam or not spam:\ndef predict_spam(email, weights):\n    features = email_to_vector(email)\n    score = weights.dot(features)\n    probability = sigmoid(score)\n    return probability &gt; 0.5\n    \n# Example usage\nweights = train_logistic_regression(train_emails, train_labels)\n\nnew_email = \"Subject: Amazing opportunity to work from home!\"\nprediction = predict_spam(new_email, weights)\n\nprint(prediction)  # Output: True\nThis process is fundamentally inductive:\n\nWe start with a collection of specific examples (the training emails and their labels).\nWe use these examples to learn a general rule (the weight vector \\(θ*\\)) for mapping from inputs to outputs.\nWe apply this rule to make predictions about new, unseen examples (e.g. classifying the new email as spam).\n\nJust as with human induction, there is no guarantee that the predictions will be correct - the learned rule is only a generalization based on the limited sample of examples in the training data. If the training set is not perfectly representative of the real distribution of emails (which it almost never is), there will necessarily be some errors and edge cases that the model gets wrong.\nHowever, if the inductive reasoning is sound - i.e. if the patterns discovered by the learning algorithm actually capture meaningful regularities in the data - then the model’s predictions will be correct more often than not. Furthermore, as we train on larger and more diverse datasets, we can expect the accuracy and robustness of the learned patterns to improve, leading to better generalization performance.\nOf course, spam classification is a relatively simple example as far as machine learning tasks go. In more complex domains like computer vision, natural language processing, or strategic decision-making, the input features and output labels can be much higher-dimensional and more abstract, the model families more elaborate and multilayered, and the optimization procedures more intricate and computationally intensive.\nHowever, the fundamental inductive reasoning remains the same:\n\nStart with a set of training examples that (hopefully) capture relevant patterns and variations.\nDefine a suitably expressive model family and objective function.\nUse an optimization algorithm to find the model parameters that minimize the objective on the training set.\nApply the learned model to predict outputs for new, unseen inputs.\nEvaluate the quality of the predictions and iterate to improve the data, model, and optimization as needed.\n\nThe power of this paradigm lies in its generality - by framing the search for patterns as an optimization problem, learning algorithms can be applied to an extremely wide range of domains and tasks without the need for detailed domain-specific knowledge engineering. Given enough data and compute, the same basic approach can be used to learn patterns in images, text, speech, sensor readings, economic trends, user behavior, and countless other types of data.\nAt the same time, the generality of the paradigm also highlights some of its limitations and challenges:\n\nDependence on data quality: The performance of a learning system is fundamentally limited by the quality and representativeness of its training data. If the data is noisy, biased, or incomplete, the learned patterns will reflect those limitations.\nOpacity of learned representations: The patterns discovered by learning algorithms can be highly complex and challenging to interpret. While simpler model families like linear regression produce relatively transparent representations, the internal structure of large neural networks is often inscrutable, making it difficult to understand how they arrive at their predictions.\nLack of explicit reasoning: Learning systems excel at discovering statistical patterns, but struggle with the kind of explicit, logical reasoning that comes naturally to humans. Tasks that require careful deliberation, causal analysis, or manipulation of symbolic representations can be challenging to frame in purely statistical terms.\nPotential for bias and fairness issues: If the training data reflects societal biases or underrepresents certain groups, the learned models can perpetuate or even amplify those biases in their predictions. Careful auditing and debiasing of data and models is essential to ensure equitable outcomes.\n\nDespite these challenges, the inductive learning paradigm has proven remarkably effective across a wide range of applications. In domains from medical diagnosis and scientific discovery to robotics and autonomous vehicles, machine learning systems are able to match or exceed human performance by discovering patterns that are too subtle or complex for manual specification. As the availability of data and computing power continues to grow, it’s likely that the scope and impact of machine learning will only continue to expand.\n\n\n6.3.2 The Role of Uncertainty\nOne of the most crucial things to understand about machine learning is that it is, at its core, a fundamentally probabilistic endeavor. When a learning algorithm draws conclusions from data, those conclusions are never absolutely certain, but rather statements of probability based on the patterns in the training examples.\nThink back to our spam classification example. Even if our training dataset was very large and diverse, covering a wide range of both spam and legitimate emails, we can never be 100% sure that the patterns it captures will hold for every possible future email. There could always be some new type of spam that looks very different from what we’ve seen before, or some unusual legitimate email that happens to share many features with typical spam.\nWhat a good machine learning model gives us, then, is not a definite classification, but a probability estimate. When we use logistic regression to predict the “spamminess” of an email, the model’s output is a number between 0 and 1 that can be interpreted as the estimated probability that the email is spam, given its input features.\nThis might seem like a limitation compared to a deterministic rule-based system that always gives a definite yes-or-no answer. However, having a principled way to quantify uncertainty is actually a key strength of the probabilistic approach. By explicitly representing the confidence of its predictions, a probabilistic model provides valuable information for downstream decision-making and risk assessment.\nFor instance, consider an email client that uses a spam filter to automatically move suspected spam messages to a separate folder. If the filter is based on a probabilistic model, we can set a confidence threshold for taking this action - say, only move messages with a 95% or higher probability of being spam. This allows us to trade off between false positives (legitimate emails moved to the spam folder) and false negatives (spam emails left in the main inbox) in a principled way.\nMore generally, having access to well-calibrated probability estimates opens up a range of possibilities for uncertainty-aware decision making, such as:\n\nDeferring to human judgment for borderline cases where the model is unsure\nGathering additional information (e.g. asking the user for feedback) to resolve uncertainty\nHedging decisions to balance risk and reward in the face of uncertain outcomes\nCombining predictions from multiple models to improve overall confidence\n\nOf course, for these benefits to be realized, it’s essential that the probability estimates produced by the model are actually well-calibrated - that is, they accurately reflect the true likelihood of the predicted outcomes. If a model consistently predicts 95% confidence for events that only occur 60% of the time, its uncertainty estimates are not reliable.\nThere are various techniques for quantifying and calibrating uncertainty in machine learning models, including:\n\nExplicit probability models: Some model families, like Bayesian networks or Gaussian processes, are designed to naturally produce probability distributions over outcomes. By incorporating prior knowledge and explicitly modeling sources of uncertainty, these approaches can provide principled uncertainty estimates.\nEnsemble methods: Techniques like bagging (bootstrap aggregating) and boosting involve training multiple models on different subsets or weightings of the data, then combining their predictions. The variation among the ensemble’s predictions provides a measure of uncertainty.\nCalibration methods: Post-hoc calibration techniques like Platt scaling or isotonic regression can be used to adjust the raw confidence scores from a model to better align with empirical probabilities.\nConformal prediction: A framework for providing guaranteed coverage rates for predictions, based on the assumption that the data are exchangeable. Conformal predictors accompany each prediction with a measure of confidence and a set of possible outcomes.\n\nThe importance of quantifying uncertainty goes beyond just improving decision quality - it’s also crucial for building trust and promoting responsible use of machine learning systems. When a model accompanies its predictions with meaningful confidence estimates, users can make informed choices about when and how to rely on its outputs. This is especially important in high-stakes domains like healthcare or criminal justice, where the consequences of incorrect predictions can be serious.\nFinally, reasoning about uncertainty is also central to more advanced machine learning paradigms like reinforcement learning and active learning:\n\nIn reinforcement learning, an agent learns to make decisions by interacting with an environment and receiving rewards or penalties. Because the environment is often stochastic and the consequences of actions are uncertain, the agent must reason about the expected long-term value of different choices under uncertainty.\nIn active learning, a model is allowed to interactively query for labels of unlabeled examples that would be most informative for improving its predictions. Selecting these examples requires estimating the expected reduction in uncertainty from obtaining their labels, based on the model’s current state of knowledge.\n\nAs we continue to push the boundaries of what machine learning systems can do, the ability to properly quantify and reason about uncertainty will only become more essential. From building robust and reliable models to enabling effective human-AI collaboration, embracing uncertainty is key to unlocking the full potential of machine learning.\n\n\n6.3.3 Model Complexity and Regularization\nAnother fundamental challenge in machine learning is striking the right balance between model complexity and generalization performance. On one hand, we want our models to be expressive enough to capture meaningful patterns in the data. On the other hand, we don’t want them to overfit to noise or idiosyncrasies of the training set and fail to generalize to new examples.\nThis tradeoff is commonly known as the bias-variance dilemma:\n\nBias refers to the error that comes from modeling assumptions and simplifications. A model with high bias makes strong assumptions about the data-generating process, which can lead to underfitting if those assumptions are wrong.\nVariance refers to the error that comes from sensitivity to small fluctuations in the training data. A model with high variance can fit the training set very well but may overfit to noise and fail to generalize to unseen examples.\n\nAs an analogy, think of trying to fit a curve to a set of scattered data points. A simple linear model has high bias but low variance - it makes the strong assumption that the relationship is linear, which limits its ability to fit complex patterns, but also makes it relatively stable across different subsets of the data. Conversely, a complex high-degree polynomial has low bias but high variance - it can fit the training points extremely well, but may wildly oscillate between them and make very poor predictions on new data.\nIn general, as we increase the complexity of a model (e.g. by adding more features, increasing the depth of a neural network, or reducing the strength of regularization), we decrease bias but increase variance. The goal is to find the sweet spot where the model is complex enough to capture relevant patterns but not so complex that it overfits to noise.\nOne way to control model complexity is through the choice of hypothesis space - the set of possible models that the learning algorithm can consider. A simple hypothesis space (like linear functions of the input features) will have low variance but potentially high bias, while a complex hypothesis space (like deep neural networks with millions of parameters) will have low bias but potentially high variance.\nAnother key tool for managing complexity is regularization - techniques for constraining the model’s parameters or limiting its capacity to overfit. Some common regularization approaches include:\n\nParameter norm penalties: Adding a penalty term to the loss function that encourages the model’s weights to be small. L2 regularization (also known as weight decay) penalizes the squared Euclidean norm of the weights, while L1 regularization penalizes the absolute values. These penalties discourage the model from relying too heavily on any one feature.\nDropout: Randomly “dropping out” (setting to zero) a fraction of the activations in a neural network during training. This prevents the network from relying too heavily on any one pathway and encourages it to learn redundant representations.\nEarly stopping: Monitoring the model’s performance on a validation set during training and stopping the optimization process when the validation error starts to increase, even if the training error is still decreasing. This prevents the model from overfitting to the training data.\n\nThe amount and type of regularization to apply is a key hyperparameter that must be tuned based on the characteristics of the data and the model. Too much regularization can lead to underfitting, while too little can lead to overfitting. Techniques like cross-validation and information criteria can help guide the selection of appropriate regularization settings.\nIt’s worth noting that the bias-variance tradeoff and the role of regularization can vary depending on the amount of training data available. In the “classical” regime where the number of examples is small relative to the number of model parameters, regularization is essential for preventing overfitting. However, in the “modern” regime of very large datasets and overparameterized models (like deep neural networks with millions of parameters), the risk of overfitting is much lower, and the role of regularization is more subtle.\nIn fact, recent research has suggested that overparameterized models can exhibit “double descent” behavior, where increasing the model complexity beyond the point of interpolating the training data can actually improve generalization performance. This challenges the classical view of the bias-variance tradeoff and suggests that our understanding of model complexity and generalization is still evolving. Despite these nuances, the basic principles of managing model complexity and using regularization to promote generalization remain central to the practice of machine learning. As we train increasingly powerful models on ever-larger datasets, finding the right balance between expressiveness and constrainedness will be key to achieving robust and reliable performance.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html#building-learning-systems",
    "href": "contents/core/10_ml_foundations.html#building-learning-systems",
    "title": "6  The Art and Science of Machine Learning",
    "section": "6.4 Building Learning Systems",
    "text": "6.4 Building Learning Systems\nNow that we’ve explored some of the key theoretical principles behind machine learning, let’s turn our attention to the practical considerations involved in building effective learning systems. What are the key components of a successful machine learning pipeline, and how do they fit together?\n\n6.4.1 Data Preparation\nThe first and arguably most important step in any machine learning project is preparing the data. As we saw in Section 6.2.2, the quality and representativeness of the training data is essential for learning meaningful patterns that generalize well to new examples. No amount of algorithmic sophistication can make up for fundamentally flawed or insufficient data.\nKey aspects of data preparation include:\n\nData cleaning: Identifying and correcting errors, inconsistencies, and missing values in the raw data. This can involve steps like removing duplicate records, standardizing formats, and imputing missing values based on statistical patterns.\nFeature engineering: Transforming the raw input data into a representation that is more amenable to learning. This can involve steps like normalizing numeric features, encoding categorical variables, extracting domain-specific features, and reducing dimensionality.\nData augmentation: Increasing the size and diversity of the training set by generating new examples through transformations of the existing data. This is especially common in domains like computer vision, where techniques like random cropping, flipping, and color jittering can help improve the robustness of the learned models.\nData splitting: Dividing the data into separate sets for training, validation, and testing. The training set is used to fit the model parameters, the validation set is used to tune hyperparameters and detect overfitting, and the test set is used to evaluate the final performance of the model on unseen data.\n\n\n\n\n\n\nThe specifics of data preparation will vary depending on the domain and the characteristics of the data, but the general principles of ensuring data quality, representativeness, and suitability for learning are universal. It’s often said that data preparation is 80% of the work in machine learning, and while this may be an exaggeration, it underscores the critical importance of getting the data right.\nTo make this more concrete, let’s consider an example of data preparation in the context of a real-world problem. Suppose we’re working on a machine learning system to predict housing prices based on features like square footage, number of bedrooms, location, etc. Our raw data might look something like this:\nAddress,Sq.Ft.,Beds,Baths,Price\n123 Main St,2000,3,2.5,$500,000\n456 Oak Ave,1500,2,1,\"$350,000\"\n789 Elm Rd,1800,3,2,425000\nTo prepare this data for learning, we might perform the following steps:\n\nStandardize the ‘Price’ column to remove the “$” and “,” characters and convert to a numeric type.\nImpute missing values in the ‘Beds’ and ‘Baths’ columns (if any) with the median or most frequent value.\nNormalize the ‘Sq.Ft.’ column by subtracting the mean and dividing by the standard deviation.\nOne-hot encode the ‘Address’ column into separate binary features for each unique location.\nSplit the data into training, validation, and test sets in a stratified fashion to ensure representative price distributions in each split.\n\nThe end result might look something like this:\nSq.Ft._Norm,Beds,Baths,123_Main_St,456_Oak_Ave,789_Elm_Rd,...,Price\n,3,2.5,1,0,0,...,500000\n-0.58,2,1,0,1,0,...,350000\n,3,2,0,0,1,...,425000\n...\nOf course, this is just a toy example, and in practice the data preparation process can be much more involved. The key point is that investing time and effort into carefully preparing the data is essential for building successful learning systems.\n\n\n6.4.2 Model Selection and Training\nOnce the data is prepared, the next step is to select an appropriate model family and training procedure. As we saw in Section 6.3.3, this involves striking a balance between model complexity and generalization ability, often through a combination of cross-validation and regularization techniques.\nSome key considerations in model selection include:\n\nInductive biases: The assumptions and constraints that are built into the model architecture. For example, convolutional neural networks have an inductive bias towards translation invariance and local connectivity, which makes them well-suited for image recognition tasks.\nParameter complexity: The number of learnable parameters in the model, which affects its capacity to fit complex patterns but also its potential to overfit to noise in the training data. Regularization techniques can help control parameter complexity.\nComputational complexity: The time and memory requirements for training and inference with the model. More complex models may require specialized hardware (like GPUs) and longer training times, which can be a practical limitation.\nInterpretability: The extent to which the learned model can be inspected and understood by humans. In some domains (like healthcare or finance), interpretability may be a key requirement for building trust and ensuring regulatory compliance.\n\nThe choice of model family will depend on the nature of the problem and the characteristics of the data. For structured data with clear feature semantics, “shallow” models like linear regression, decision trees, or support vector machines may be appropriate. For unstructured data like images, audio, or text, “deep” models like convolutional or recurrent neural networks are often used.\nOnce a model family is selected, the next step is to train the model on the prepared data. This typically involves the following steps:\n\nInstantiate the model with initial parameter values (e.g. random weights for a neural network).\nDefine a loss function that measures the discrepancy between the model’s predictions and the true labels on the training set.\nUse an optimization algorithm (like stochastic gradient descent) to iteratively update the model parameters to minimize the loss function.\nMonitor the model’s performance on the validation set to detect overfitting and tune hyperparameters.\nStop training when the validation performance plateaus or starts to degrade.\n\nThe specifics of the training process will vary depending on the chosen model family and optimization algorithm, but the general goal is to find the model parameters that minimize the empirical risk on the training data while still generalizing well to unseen examples.\nTo illustrate these ideas, let’s continue with our housing price prediction example. Suppose we’ve decided to use a regularized linear regression model of the form:\n\\[price = w_0 + w_1 * sqft + w_2 * beds + w_3 * baths + ...\\]\nwhere \\(w_0, w_1, ...\\) are the learned weights and sqft, beds, baths, ... are the input features.\nWe can train this model using gradient descent on the mean squared error loss function:\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef gradient_descent(X, y, w, lr=0.01, num_iters=100):\n    for i in range(num_iters):\n        y_pred = np.dot(X, w)\n        error = y_pred - y\n        gradient = 2 * np.dot(X.T, error) / len(y)\n        w -= lr * gradient\n    return w\n\n# Add a bias term to the feature matrix\nX = np.c_[np.ones(len(X)), X]  \n\n# Initialize weights to zero\nw = np.zeros(X.shape[1])\n\n# Train the model\nw = gradient_descent(X, y, w)\nWe can also add L2 regularization to the loss function to prevent overfitting:\ndef mse_loss_regularized(y_true, y_pred, w, alpha=0.01):\n    return mse_loss(y_true, y_pred) + alpha * np.sum(w**2)\n\ndef gradient_descent_regularized(X, y, w, lr=0.01, alpha=0.01, num_iters=100):\n    for i in range(num_iters):\n        y_pred = np.dot(X, w)\n        error = y_pred - y\n        gradient = 2 * np.dot(X.T, error) / len(y) + 2 * alpha * w\n        w -= lr * gradient\n    return w\n\n# Train the regularized model\nw = gradient_descent_regularized(X, y, w)\nThe alpha parameter controls the strength of the regularization - larger values will constrain the weights more strongly, while smaller values will allow the model to fit the training data more closely.\nBy tuning the learning rate lr, regularization strength alpha, and number of iterations num_iters, we can find the model that achieves the best balance between fitting the training data and generalizing to new examples.\nOf course, linear regression is just one possible model choice for this problem. We could also experiment with more complex models like decision trees, random forests, or neural networks, each of which would have its own set of hyperparameters to tune. The key is to use a combination of domain knowledge, empirical validation, and iterative refinement to find the model that best suits the problem at hand.\n\n\n6.4.3 Model Evaluation and Deployment\nOnce we’ve trained a model that performs well on the validation set, the final step is to evaluate its performance on the held-out test set. This gives us an unbiased estimate of how well the model is likely to generalize to real-world data.\nCommon evaluation metrics for classification problems include:\n\nAccuracy: The fraction of examples that are correctly classified.\nPrecision: The fraction of positive predictions that are actually positive.\nRecall: The fraction of actual positives that are predicted positive.\nF1 Score: The harmonic mean of precision and recall.\nROC AUC: The area under the receiver operating characteristic curve, which measures the tradeoff between true positive rate and false positive rate.\n\nFor regression problems, common metrics include:\n\nMean squared error (MSE): The average squared difference between the predicted and actual values.\nMean absolute error (MAE): The average absolute difference between the predicted and actual values.\nR-squared (R²): The proportion of variance in the target variable that is predictable from the input features.\n\nIt’s important to choose an evaluation metric that aligns with the business goals of the problem. For example, in a fraud detection system, we might care more about recall (catching as many fraudulent transactions as possible) than precision (avoiding false alarms), while in a medical diagnosis system, we might care more about precision (avoiding false positives that could lead to unnecessary treatments).\nIf the model’s performance on the test set is satisfactory, we can proceed to deploy it in a production environment. This involves integrating the trained model into a larger software system that can apply it to new input data and surface the predictions to end users.\nSome key considerations in model deployment include:\n\nScalability: Can the model handle the volume and velocity of data in the production environment? This may require techniques like batch processing, streaming, or distributed computation.\nLatency: How quickly does the model need to generate predictions in order to meet business requirements? This may require optimizations like model compression, quantization, or hardware acceleration.\nMonitoring: How will the model’s performance be monitored and maintained over time? This may involve tracking key metrics, detecting data drift, and periodically retraining the model on fresh data.\nSecurity: How will the model and its predictions be protected from abuse or unauthorized access? This may involve techniques like input validation, output filtering, or access controls.\n\nDeploying and maintaining machine learning models in production is a complex topic that requires close collaboration between data scientists, software engineers, and domain experts. It’s an active area of research and development, with new tools and best practices emerging regularly.\nTo bring everything together, let’s return one last time to our housing price prediction example. After training and validating our regularized linear regression model, we can evaluate its performance on the test set:\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Generate predictions on the test set\ny_pred = np.dot(X_test, w)\n\n# Calculate evaluation metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Test MSE: {mse:.2f}\")\nprint(f\"Test MAE: {mae:.2f}\")\nprint(f\"Test R^2: {r2:.2f}\")\nIf we’re satisfied with the model’s performance, we can deploy it as part of a larger housing price estimation service. This might involve:\n\nWrapping the trained model in a web service API that can accept new housing features and return price predictions.\nIntegrating the API with a user-facing application that allows homeowners or real estate agents to input property information and receive estimates.\nSetting up a data pipeline to continuously collect new housing data and periodically retrain the model to capture changing market conditions.\nDefining monitoring dashboards and alerts to track the model’s performance over time and detect any anomalies or degradations.\nEstablishing governance policies and processes for managing the lifecycle of the model, from development to retirement.\n\nAgain, this is a simplified example, but it illustrates the end-to-end process of building a machine learning system, from data preparation to model development to deployment and maintenance.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html#the-ethics-and-governance-of-machine-learning",
    "href": "contents/core/10_ml_foundations.html#the-ethics-and-governance-of-machine-learning",
    "title": "6  The Art and Science of Machine Learning",
    "section": "6.5 The Ethics and Governance of Machine Learning",
    "text": "6.5 The Ethics and Governance of Machine Learning\nAs machine learning systems become more prevalent and powerful, it’s crucial that we grapple with the ethical implications of their development and deployment. In this final section, we’ll explore some key ethical considerations and governance principles for responsible machine learning.\n\n6.5.1 Fairness and Bias\nOne of the most pressing ethical challenges in machine learning is ensuring that models are fair and unbiased. If the training data reflects historical biases or discrimination, the resulting model may perpetuate or even amplify those biases in its predictions.\nFor example, consider a hiring model that is trained on past hiring decisions to predict the likelihood of a candidate being successful in a job. If the training data comes from a company with a history of discriminatory hiring practices, the model may learn to penalize candidates from underrepresented groups, even if those factors are not actually predictive of job performance.\nDetecting and mitigating bias in machine learning systems is an active area of research, with techniques like:\n\nDemographically balancing datasets to ensure equal representation of different groups\nAdversarial debiasing to remove sensitive information from model representations\nRegularization techniques to penalize models that exhibit disparate impact\nPost-processing methods to adjust model outputs to satisfy fairness constraints\n\nHowever, these techniques are not perfect, and there is often a tradeoff between fairness and accuracy. Moreover, fairness is not a purely technical issue, but a sociotechnical one that requires ongoing collaboration between machine learning practitioners, domain experts, policymakers, and affected communities.\n\n\n6.5.2 Transparency and Accountability\nAnother key ethical principle for machine learning is transparency and accountability. As models become more complex and consequential, it becomes harder for humans to understand how they arrive at their predictions and to trace the provenance of their training data and design choices.\nThis opacity can make it difficult to audit models for bias, safety, or compliance with regulations. It can also make it harder to challenge or appeal decisions made by machine learning systems, leading to a loss of human agency and recourse.\nSome techniques for promoting transparency and accountability in machine learning include:\n\nModel interpretability methods that provide human-understandable explanations of model predictions\nProvenance tracking to document the lineage of data, code, and models used in a system\nAudit trails and version control to enable reproducibility and historical analysis\nParticipatory design processes that involve affected stakeholders in the development and governance of models\n\nHowever, like fairness, transparency is not purely a technical problem. It also requires institutional structures and processes for oversight, redress, and accountability. This might involve things like:\n\nDesignating responsible individuals or teams for the ethical development and deployment of machine learning systems\nEstablishing review boards or oversight committees to assess the social impact and governance of models\nCreating channels for affected individuals and communities to provide input and feedback on the use of machine learning in their lives\nDeveloping legal and regulatory frameworks to enforce transparency and accountability standards\n\n\n\n6.5.3 Safety and Robustness\nAs machine learning systems are deployed in increasingly high-stakes domains, from healthcare to transportation to criminal justice, ensuring their safety and robustness becomes paramount. Models that are brittle, unreliable, or easily fooled can lead to serious harms if they are not carefully designed and tested.\nSome key challenges in machine learning safety and robustness include:\n\nDistributional shift, where models trained on one data distribution may fail unexpectedly when applied to a different distribution\nAdversarial attacks, where malicious actors can craft inputs that fool models into making egregious errors\nReward hacking, where optimizing for the wrong objective function can lead models to behave in unintended and harmful ways\nSafe exploration, where models need to learn about their environment without taking catastrophic actions\n\nTechniques for improving the safety and robustness of machine learning systems include:\n\nAnomaly and out-of-distribution detection to flag inputs that are far from the training data\nAdversarial training and robustness regularization to make models more resilient to perturbations\nConstrained optimization and safe reinforcement learning to respect safety boundaries during learning\nFormal verification and testing to provide guarantees about model behavior under different conditions\n\nHowever, building truly safe and robust machine learning systems requires more than just technical solutions. It also requires:\n\nRigorous safety culture and practices throughout the development and deployment lifecycle\nClose collaboration between machine learning practitioners, domain experts, and safety professionals\nProactive engagement with policymakers and the public to align the development of machine learning with societal values and expectations\nOngoing monitoring and adjustment of deployed systems to catch and correct errors and unintended consequences\n\n\n\n6.5.4 Ethical Principles and Governance Frameworks\n\n\n\n\n\nTo navigate the complex ethical landscape of machine learning, we need clear principles and governance frameworks to guide responsible development and deployment. Some key principles that have been proposed include:\n\nTransparency: Machine learning systems should be auditable and understandable by humans.\nAccountability: There should be clear mechanisms for oversight, redress, and enforcement.\nFairness: Machine learning should treat all individuals equitably and avoid discriminatory impacts.\nSafety: Machine learning systems should be reliable, robust, and safe throughout their lifecycle.\nPrivacy: The collection and use of data for machine learning should respect individual privacy rights and provide appropriate protections.\nHuman agency: Machine learning systems should respect human autonomy and dignity, and provide meaningful opportunities for human input and control.\nSocietal benefit: The development and deployment of machine learning should be guided by considerations of social good and collective wellbeing.\n\nTranslating these high-level principles into practical governance frameworks is an ongoing challenge, but some key elements include:\n\nEthical codes of conduct and professional standards for machine learning practitioners\nImpact assessment and risk management processes to identify and mitigate potential harms\nStakeholder engagement and participatory design to ensure affected communities have a voice\nRegulatory sandboxes and policy experiments to test new governance approaches\nInternational standards and coordination to address the global nature of machine learning development\n\nUltimately, the goal of machine learning governance should be to ensure that the technology is developed and deployed in a way that aligns with human values and enhances, rather than undermines, human flourishing. This is a complex and ongoing process that will require sustained collaboration across disciplines, sectors, and geographies.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/10_ml_foundations.html#conclusion",
    "href": "contents/core/10_ml_foundations.html#conclusion",
    "title": "6  The Art and Science of Machine Learning",
    "section": "6.6 Conclusion",
    "text": "6.6 Conclusion\nIn this chapter, we’ve embarked on a comprehensive exploration of the foundations of machine learning, from its historical roots to its modern techniques to its future challenges. We’ve seen how the field has evolved from rule-based expert systems to data-driven statistical learning, powered by the explosion of big data and computing power. We’ve examined the fundamental components of machine learning systems - the data they learn from, the patterns they aim to extract, and the algorithms that power the learning process. We’ve discussed key concepts like inductive bias, generalization, overfitting, and regularization, and how they relate to the art of building effective models. We’ve walked through the practical steps of constructing a machine learning pipeline, from data preparation to model selection to deployment and monitoring. And we’ve grappled with some of the ethical challenges and governance principles that arise when building systems that can have significant impact on people’s lives.\nThe field of machine learning is still rapidly evolving, with new breakthroughs and challenges emerging every year. As we look to the future, some of the key frontiers and open questions include:\n\nContinual and lifelong learning: How can we build models that can learn continuously and adapt to new tasks and domains over time, without forgetting what they’ve learned before?\nCausality and interpretability: How can we move beyond purely associational patterns to uncover causal relationships and build models that are more interpretable and explainable to humans?\nRobustness and safety: How can we guarantee that machine learning systems will behave safely and reliably, even in the face of distributional shift, adversarial attacks, or unexpected situations?\nHuman-AI collaboration: How can we design machine learning systems that augment and empower human intelligence, rather than replacing or undermining it?\nEthical alignment: How can we ensure that the development and deployment of machine learning aligns with human values and promotes beneficial outcomes for society as a whole?\n\nAdvancing machine learning requires collaboration across disciplines—from computer science and statistics to psychology, social science, philosophy, and ethics. It also demands engagement with policymakers, industry leaders, and the public to ensure responsible and inclusive development. The goal is to build systems that learn from experience to make decisions that benefit humanity, whether in healthcare, scientific discovery, or improving daily life.\nHowever, realizing this potential goes beyond technical progress; it requires addressing fairness, accountability, transparency, and safety while navigating ethical and governance challenges. Machine learning practitioners must not only push technological boundaries but also consider the broader impact of their work. By embracing diverse perspectives and collaborating beyond our field, we shape the future of AI. Staying curious, critical, and committed to responsible development will ensure machine learning serves society for generations to come.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Art and Science of Machine Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/6_gesturerecog_realtime.html",
    "href": "contents/core/6_gesturerecog_realtime.html",
    "title": "7  Real-Time Gesture Recognition",
    "section": "",
    "text": "7.1 Introduction to Edge AI in Embedded Systems\nEdge AI refers to deploying AI models on edge devices, such as microcontrollers, where data is processed locally instead of being sent to a centralized cloud server. This paradigm reduces latency, enhances data privacy, and ensures uninterrupted operation even in environments with limited connectivity.\nAs embedded systems become more advanced, the need for efficient on-device data processing grows. Traditional systems rely heavily on cloud infrastructure, which can introduce latency, data privacy concerns, and increased operational costs. Edge AI addresses these issues by allowing computations to occur on the microcontroller itself, ensuring responsiveness and independence from network stability. With microcontrollers like the EFR32XG24, AI algorithms are executed efficiently despite hardware and memory constraints.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Real-Time Gesture Recognition</span>"
    ]
  },
  {
    "objectID": "contents/core/6_gesturerecog_realtime.html#introduction-to-edge-ai-in-embedded-systems",
    "href": "contents/core/6_gesturerecog_realtime.html#introduction-to-edge-ai-in-embedded-systems",
    "title": "7  Real-Time Gesture Recognition",
    "section": "",
    "text": "// Example: Initialize Edge AI on EFR32XG24\nvoid initEdgeAI() {\n    configureClock();\n    enableAIAccelerator();\n    loadAIModel();\n    initializeSensors();\n}\n\ninitEdgeAI();\n\n7.1.1 Advantages of Edge AI\nEdge AI offers several critical advantages that make it an essential technology for embedded systems:\n\nLow Latency: Immediate processing without reliance on cloud servers.\nImproved Privacy: Sensitive data remains on the device.\nReduced Bandwidth Usage: No need for constant data transmission.\nEnergy Efficiency: Optimized AI models reduce power consumption.\nScalability: Multiple devices can operate independently.\nOffline Operation: Systems continue functioning without an active internet connection.\n\nThese benefits are especially important in applications like gesture recognition, where real-time response is crucial for effective interaction. Devices deployed in remote or resource-limited environments can operate reliably without relying on continuous cloud access.\n// Example: Optimizing AI Model\nvoid optimizeAIModel() {\n    reducePrecision();\n    quantizeWeights();\n    minimizeMemoryFootprint();\n}\n\noptimizeAIModel();\n\n\n7.1.2 Why EFR32XG24 for Edge AI?\nThe EFR32XG24 microcontroller, equipped with an ARM Cortex-M33 core and integrated BLE capabilities, provides an ideal platform for Edge AI applications. Its features include:\n\nSupport for TinyML frameworks.\nDedicated hardware accelerators for AI computations.\nEnergy-efficient architecture for battery-operated devices.\nAdvanced security features for data integrity.\nHigh-speed BLE communication for real-time data transfer.\nIntegrated peripherals for sensor interfacing.\n\n\n// Example: BLE Initialization for Edge AI\nvoid initBLE() {\n    BLE_init();\n    BLE_enable();\n    BLE_setMode(BLE_LOW_POWER);\n}\n\ninitBLE();\nFurthermore, the microcontroller’s native support for TensorFlow Lite for Microcontrollers (TFLM) allows seamless deployment of lightweight AI models. Its power efficiency ensures prolonged operational life.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Real-Time Gesture Recognition</span>"
    ]
  },
  {
    "objectID": "contents/core/6_gesturerecog_realtime.html#gesture-recognition-system-overview",
    "href": "contents/core/6_gesturerecog_realtime.html#gesture-recognition-system-overview",
    "title": "7  Real-Time Gesture Recognition",
    "section": "7.2 Gesture Recognition System Overview",
    "text": "7.2 Gesture Recognition System Overview\nGesture recognition systems are a subset of human-computer interaction technologies that allow users to control and interact with devices using physical gestures. In an embedded context, gesture recognition systems aim to interpret motion patterns captured by sensors like IMUs (Inertial Measurement Units). The EFR32XG24 microcontroller enables real-time gesture recognition while maintaining energy efficiency and responsiveness.\n\n7.2.1 System Components\nA gesture recognition system using the EFR32XG24 microcontroller involves several key components:\n\nSensors: Inertial Measurement Unit (IMU) for capturing motion data.\nAI Model: A lightweight Convolutional Neural Network (CNN) optimized for TinyML.\nData Preprocessing: Noise filtering and segmentation.\nBLE Communication: Real-time data transfer to mobile devices.\nPower Management System: Ensures long battery life.\n\n\n// Example: Read IMU Sensor Data\nfloat readIMU() {\n    float x = IMU_getX();\n    float y = IMU_getY();\n    float z = IMU_getZ();\n    return (x + y + z) / 3;\n}\n\n\n7.2.2 System Workflow\nThe overall workflow of a gesture recognition system includes the following stages:\n\nRaw sensor data is captured using IMU sensors.\nData is preprocessed locally to remove noise.\nPreprocessed data is fed into the AI model.\nThe AI model classifies the gesture.\nResults are sent via BLE to a connected device.\nFeedback is displayed in real-time on a mobile or desktop application.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Real-Time Gesture Recognition</span>"
    ]
  },
  {
    "objectID": "contents/core/6_gesturerecog_realtime.html#ai-model-design-for-gesture-recognition",
    "href": "contents/core/6_gesturerecog_realtime.html#ai-model-design-for-gesture-recognition",
    "title": "7  Real-Time Gesture Recognition",
    "section": "7.3 AI Model Design for Gesture Recognition",
    "text": "7.3 AI Model Design for Gesture Recognition\nThe AI model for gesture recognition is implemented using a TinyML-compatible CNN architecture.\n\n7.3.1 Model Architecture\nThe CNN architecture is carefully designed to balance accuracy, memory consumption, and computational efficiency:\n\nInput Layer: Processes time-series IMU data.\nConvolutional Layers: Extract spatial and temporal patterns.\nDropout Layers: Prevent overfitting.\nFully Connected Layer: Classifies gestures.\nSoftmax Layer: Provides final classification probabilities.\n\n\n// Example: Preprocessing IMU Data\nvoid preprocessIMUData(float* data, int size) {\n    for (int i = 0; i &lt; size; i++) {\n        data[i] = normalize(data[i]);\n    }\n}\n\nTable 6.1: AI Model Layers and Parameters\n\n\nLayer\nType\nOutput Shape\n\n\n\n\nInput\nTime-Series Data\n(128, 3, 1)\n\n\nConv2D\nFeature Extraction\n(64, 128, 3, 8)\n\n\nMaxPooling2D\nDownsampling\n(32, 64, 8)\n\n\nDropout\nRegularization\n-\n\n\nFlatten\nVectorization\n(512)\n\n\nDense\nClassification\n(16)\n\n\nOutput\nSoftmax\n(4)",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Real-Time Gesture Recognition</span>"
    ]
  },
  {
    "objectID": "contents/core/6_gesturerecog_realtime.html#sec:methodology",
    "href": "contents/core/6_gesturerecog_realtime.html#sec:methodology",
    "title": "7  Real-Time Gesture Recognition",
    "section": "7.4 Methodology",
    "text": "7.4 Methodology\nThis section outlines the methodology used to design and implement an Edge AI-based gesture recognition system on the EFR32XG24 BLE microcontroller. The methodology consists of four main stages: Data Acquisition, Data Preprocessing, AI Model Development, and System Integration. Each stage is elaborated below.\n\n7.4.1 Data Acquisition\nThe gesture recognition system utilizes an Inertial Measurement Unit (IMU) sensor to capture motion data. The IMU consists of accelerometers, gyroscopes, and magnetometers to measure linear acceleration, angular velocity, and orientation, respectively.\nSensor Configuration:\n\nSensor Type: 6-axis IMU sensor.\nSampling Rate: 50 Hz.\nData Format: Time-series data with X, Y, and Z-axis readings.\n\nThe IMU sensor outputs raw motion data, which is collected in real-time and fed into the microcontroller for preprocessing.\n\n\n7.4.2 Data Preprocessing\nRaw data from the IMU sensor is noisy and requires preprocessing before being fed into the AI model. The preprocessing pipeline includes the following steps:\n\nNoise Filtering: A low-pass filter is applied to remove high-frequency noise.\nNormalization: Sensor readings are normalized to a common scale between 0 and 1.\nSegmentation: The data is divided into fixed-size time windows for analysis.\n\nThe preprocessed data ensures consistency and reduces variability, enabling robust AI model performance.\n\n\n7.4.3 AI Model Development\nThe AI model is implemented using a TinyML-compatible Convolutional Neural Network (CNN). The architecture is optimized for low memory and computational constraints typical of embedded devices.\nModel Architecture:\n\nInput Layer: Accepts preprocessed IMU time-series data.\nConvolutional Layers: Extract spatial and temporal patterns.\nPooling Layers: Reduce dimensionality while retaining critical features.\nDropout Layers: Prevent overfitting during training.\nFully Connected Layers: Map learned features to output classes.\nOutput Layer: Softmax layer providing probabilities for each gesture class.\n\nTraining and Optimization:\n\nFramework: TensorFlow Lite for Microcontrollers (TFLM).\nTraining Dataset: Recorded gesture datasets.\nOptimization Techniques: Weight quantization, reduced precision arithmetic, and model pruning.\n\nThe model was trained offline on a high-performance server and deployed onto the EFR32XG24 microcontroller after optimization.\n\n\n7.4.4 System Integration\nThe final deployment involves integrating the AI model with the EFR32XG24 microcontroller and establishing BLE communication for data transfer and feedback display.\nSystem Workflow:\n\nIMU sensors capture real-time motion data.\nData preprocessing is performed locally on the microcontroller.\nThe preprocessed data is passed to the AI model for gesture classification.\nResults are transmitted via BLE to connected mobile or desktop applications.\nFeedback is displayed in real-time.\n\nPower Management:\n\nAdaptive power management is implemented to minimize battery consumption.\nLow-power BLE mode is enabled for data transmission.\n\n\n\n7.4.5 Evaluation Metrics\nThe system’s performance was evaluated using the following metrics:\n\nAccuracy: Percentage of correct gesture classifications.\nLatency: Time taken for end-to-end gesture recognition.\nPower Consumption: Average energy used per gesture recognition cycle.\n\n\n\n7.4.6 Hardware and Software Tools\n\nHardware: EFR32XG24 BLE microcontroller, IMU sensor module.\nSoftware: TensorFlow Lite for Microcontrollers, Embedded C, BLE SDK.\n\nThe methodology ensures an efficient, real-time, and scalable gesture recognition system optimized for embedded hardware constraints.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Real-Time Gesture Recognition</span>"
    ]
  },
  {
    "objectID": "contents/core/6_gesturerecog_realtime.html#sec:challenges",
    "href": "contents/core/6_gesturerecog_realtime.html#sec:challenges",
    "title": "7  Real-Time Gesture Recognition",
    "section": "7.5 Challenges and Limitations",
    "text": "7.5 Challenges and Limitations\nWhile implementing the Edge AI-based gesture recognition system on the EFR32XG24 microcontroller, several challenges and limitations were encountered. These are discussed below:\n\nHardware Constraints: The limited computational power and memory resources of the microcontroller posed restrictions on the complexity and size of the AI model. Optimizing the AI model for memory efficiency while maintaining accuracy was a significant challenge.\nPower Consumption: Real-time gesture recognition is computationally intensive, and ensuring prolonged battery life for portable devices required careful power management strategies.\nSensor Noise: IMU sensors are susceptible to noise and environmental disturbances, which can introduce inaccuracies in gesture data. Filtering and preprocessing techniques had to be carefully designed to address this issue.\nLatency Constraints: Edge AI systems require minimal latency for real-time performance. Achieving low latency while balancing model accuracy and computational load was challenging.\nBLE Communication Bottlenecks: Real-time data transfer via BLE can be affected by interference, limited bandwidth, and power constraints, impacting system responsiveness.\nScalability:** Scaling the system for multi-gesture recognition or increasing the number of edge devices posed integration challenges due to resource limitations.\nEnvironmental Variability: Gesture recognition accuracy can degrade under varying environmental conditions, such as lighting changes, device orientation, or sudden movements.\n\nAddressing these challenges required a combination of hardware optimization, software fine-tuning, and iterative testing to ensure a balance between performance, accuracy, and efficiency.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Real-Time Gesture Recognition</span>"
    ]
  },
  {
    "objectID": "contents/core/7_gesturerecog_magicwand.html",
    "href": "contents/core/7_gesturerecog_magicwand.html",
    "title": "8  Magic Wand via Gesture Recognition",
    "section": "",
    "text": "8.1 System Overview\nA convolutional neural network (CNN) model was trained in this TinyML project to recognize gestures such as Swipe Up, Swipe Down, and Circle based on the onboard accelerometer data. The detected gestures (Arrow Up, Arrow Down, and Play/Pause) are mapped to media control actions and transmitted over BLE as media key presses and over the UART interface. Figure 7.1 shows an instance of the output.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**Magic Wand via Gesture Recognition**</span>"
    ]
  },
  {
    "objectID": "contents/core/7_gesturerecog_magicwand.html#system-overview",
    "href": "contents/core/7_gesturerecog_magicwand.html#system-overview",
    "title": "8  Magic Wand via Gesture Recognition",
    "section": "",
    "text": "Figure 7.1: Sample Data Output via UART Terminal",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**Magic Wand via Gesture Recognition**</span>"
    ]
  },
  {
    "objectID": "contents/core/7_gesturerecog_magicwand.html#data-collection-and-processing",
    "href": "contents/core/7_gesturerecog_magicwand.html#data-collection-and-processing",
    "title": "8  Magic Wand via Gesture Recognition",
    "section": "8.2 Data Collection and Processing",
    "text": "8.2 Data Collection and Processing\nThe IMU data for this TinyML model was collected at 25Hz with a sequence length of 50 samples to form the input buffer for the CNN. The fastest way to do this is by following the github.com/Ijnaka22len/FastDataCollection4MagicWangProjects repository, which provides a detailed description of IMU data capture for TinyML projects. The captured data is preprocessed and fed into the model as an input image for pattern recognition.\nif __name__ == \"__main__\":\n    delFile(file_path=\"data/complete_data\")\n    delFile(file_path=\"data/test\")\n    delFile(file_path=\"data/train\")\n    delFile(file_path=\"data/valid\")\n    delFile(file_path=\"netmodels/CNN/weights.h5\")\n\n    folders = os.listdir(\"data\")\n    names = [file.split('_')[1].lower() for file in os.listdir(f\"data/{folders[0]}\")]\n    data = []  # pylint: disable=redefined-outer-name\n\n    for idx1, folder in enumerate(folders):\n        files = os.listdir(f\"data/{folder}\")\n        for file in files:\n            name = file.split('_')[1].lower()\n            prepare_original_data(folder, name, data, f\"data/{folder}/{file}\")\n\n    for idx in range(5):\n        prepare_original_data(\"negative\", \"negative%d\" % (idx + 1), data, \"data/negative/negative_%d.txt\" % (idx + 1))\n    \n    generate_negative_data(data)\n    print(\"data_length: \" + str(len(data)))\n\n    if not os.path.exists(\"./data\"):\n        os.makedirs(\"./data\")\n    \n    write_data(data, \"./data/complete_data\")",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**Magic Wand via Gesture Recognition**</span>"
    ]
  },
  {
    "objectID": "contents/core/7_gesturerecog_magicwand.html#model-architecture-and-deployment",
    "href": "contents/core/7_gesturerecog_magicwand.html#model-architecture-and-deployment",
    "title": "8  Magic Wand via Gesture Recognition",
    "section": "8.3 Model Architecture and Deployment",
    "text": "8.3 Model Architecture and Deployment\nTensorFlow Lite is used to create the CNN model, which takes the processed IMU data as an input image for multiclass classification of the various classes. The AIDrawPen chapter can be reviewed to show how to train a TinyML model for this microcontroller.\n\"\"\"Trains the model.\"\"\"\ncalculate_model_size(model)\nepochs = 50\nbatch_size = 64\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nif kind == \"CNN\":\n    train_data = train_data.map(reshape_function)\n    test_data = test_data.map(reshape_function)\n    valid_data = valid_data.map(reshape_function)\n\ntest_labels = np.zeros(test_len)\nidx = 0\nfor data, label in test_data:  # pylint: disable=unused-variable\n    test_labels[idx] = label.numpy()\n    idx += 1\n\ntrain_data = train_data.batch(batch_size).repeat()\nvalid_data = valid_data.batch(batch_size)\ntest_data = test_data.batch(batch_size)\n\nhistory = model.fit(train_data,\n              epochs=epochs,\n              validation_data=valid_data,\n              steps_per_epoch=1000,\n              validation_steps=int((valid_len - 1) / batch_size + 1),\n              callbacks=[tensorboard_callback, early_stop, checkpoint])\n\nloss, acc = model.evaluate(test_data)\npred = np.argmax(model.predict(test_data), axis=1)\nconfusion = tf.math.confusion_matrix(labels=tf.constant(test_labels),\n                                     predictions=tf.constant(pred),\n                                     num_classes=4)",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**Magic Wand via Gesture Recognition**</span>"
    ]
  },
  {
    "objectID": "contents/core/7_gesturerecog_magicwand.html#firmware-and-model-inference",
    "href": "contents/core/7_gesturerecog_magicwand.html#firmware-and-model-inference",
    "title": "8  Magic Wand via Gesture Recognition",
    "section": "8.4 Firmware and Model Inference",
    "text": "8.4 Firmware and Model Inference\nThe microcontroller firmware contains a code segment to capture the accelerometer data at the predetermined frequency and sampling rate. This data is stored in a buffer and updated every 100ms during gesture detection.\n#include \"accelerometer.h\"\n#include \"config.h\"\n\n#if defined(SL_COMPONENT_CATALOG_PRESENT)\n#include \"sl_component_catalog.h\"\n#endif\n\n#if defined (SL_CATALOG_ICM20689_DRIVER_PRESENT)\n#include \"sl_icm20689_config.h\"\n#define  SL_IMU_INT_PORT SL_ICM20689_INT_PORT\n#define  SL_IMU_INT_PIN  SL_ICM20689_INT_PIN\n#elif defined (SL_CATALOG_ICM20648_DRIVER_PRESENT)\n#include \"sl_icm20648_config.h\"\n#define  SL_IMU_INT_PORT SL_ICM20648_INT_PORT\n#define  SL_IMU_INT_PIN  SL_ICM20648_INT_PIN\n#else\n#error \"No IMU driver defined\"\n#endif\n\n// Accelerometer data from sensor\ntypedef struct imu_data {\n  int16_t x;\n  int16_t y;\n  int16_t z;\n} imu_data_t;\n\nsl_status_t accelerometer_setup(GPIOINT_IrqCallbackPtrExt_t callbackPtr)\n{\n  sl_status_t status = SL_STATUS_OK;\n  int int_id;\n\n  // Initialize accelerometer sensor\n  status = sl_imu_init();\n  if (status != SL_STATUS_OK) {\n    return status;\n  }\n  sl_imu_configure(ACCELEROMETER_FREQ);\n  // Setup interrupt from accelerometer on falling edge\n  GPIO_PinModeSet(SL_IMU_INT_PORT, SL_IMU_INT_PIN, gpioModeInput, 0);\n  int_id = GPIOINT_CallbackRegisterExt(SL_IMU_INT_PIN, callbackPtr, 0);\n  if (int_id != INTERRUPT_UNAVAILABLE) {\n    GPIO_ExtIntConfig(SL_IMU_INT_PORT, SL_IMU_INT_PIN, int_id, false, true, true);\n  } else {\n    status = SL_STATUS_FAIL;\n  }\n  return status;\n}\n\nsl_status_t accelerometer_read(acc_data_t* dst)\n{\n  if (!sl_imu_is_data_ready()) {\n    return SL_STATUS_FAIL;\n  }\n  sl_imu_update();\n  int16_t m[3];\n  sl_imu_get_acceleration(m);\n  CORE_DECLARE_IRQ_STATE;\n  CORE_ENTER_CRITICAL();\n  if (dst != NULL) {\n    dst-&gt;x = m[0];\n    dst-&gt;y = m[1];\n    dst-&gt;z = m[2];\n  }\n  CORE_EXIT_CRITICAL();\n  return SL_STATUS_OK;\n}\nThe quantized CNN model interprets the processed accelerometer data to classify gestures through periodic inference. Results are evaluated against the accepted threshold (#define DETECTION_THRESHOLD 0.9f).\n#include \"sl_tflite_micro_model.h\"\n#include \"sl_tflite_micro_init.h\"\n#include \"sl_sleeptimer.h\"\n#include \"magic_wand.h\"\n#include \"accelerometer.h\"\n#include \"sl_simple_button_instances.h\"\n#include \"math.h\"\n#include \"config.h\"\n// BLE header\n#include \"sl_bluetooth.h\"\n#include \"app_assert.h\"\n#include \"gatt_db.h\"\n#include \"em_common.h\"\n//\nstatic int input_length;\nstatic TfLiteTensor* model_input;\nstatic tflite::MicroInterpreter* interpreter;\nstatic acc_data_t buf[SEQUENCE_LENGTH] = { 0.5f, 0.5f, 0.5f };\nstatic bool infer = false;\nstatic bool read_accel = false;\nstatic int head_ptr = 0;\nstatic int inference_trigger_samples_num = round(INFERENCE_PERIOD_MS / ACCELEROMETER_FREQ);\nstatic acc_data_t prev_data = { 0.5f, 0.5f, 0.5f };\n\nstatic void listen_for_gestures(bool enable)\n{\n  if (enable) {\n    for (uint8_t i = 0; i &lt; SEQUENCE_LENGTH; i++) {\n      acc_data_t _d = { 0.5f, 0.5f, 0.5f };\n      buf[i] = _d;\n    }\n    read_accel = true;\n  } else {\n    read_accel = false;\n    head_ptr = 0;\n  }\n}\n\nvoid sl_button_on_change(const sl_button_t *handle)\n{\n  if (sl_button_get_state(handle) == SL_SIMPLE_BUTTON_PRESSED) {\n    if (&sl_button_btn0 == handle) {\n      listen_for_gestures(true);\n    }\n  } else if (sl_button_get_state(handle) == SL_SIMPLE_BUTTON_RELEASED) {\n    if (&sl_button_btn0 == handle) {\n      listen_for_gestures(false);\n    }\n  }\n}\n\n// Called when the IMU has data available using gpio interrupt.\nstatic void on_data_available(uint8_t int_id, void *ctx)\n{\n  (void) int_id;\n  (void) ctx;\n  acc_data_t data = { 0, 0, 0 };\n  sl_status_t status = accelerometer_read(&data);\n  if (status == SL_STATUS_FAIL || !read_accel) {\n    return;\n  }\n\n  data.x /= 2000;\n  data.y /= 2000;\n  data.z /= 2000;\n\n  acc_data_t delta_data = { 0 };\n  delta_data.x = data.x - prev_data.x;\n  delta_data.y = data.y - prev_data.y;\n  delta_data.z = data.z - prev_data.z;\n\n  delta_data.x = (delta_data.x / 2 + 1) / 2;\n  delta_data.y = (delta_data.y / 2 + 1) / 2;\n  delta_data.z = (delta_data.z / 2 + 1) / 2;\n\n  buf[head_ptr].x = delta_data.x;\n  buf[head_ptr].y = delta_data.y;\n  buf[head_ptr].z = delta_data.z;\n\n  head_ptr++;\n  prev_data.x = data.x;\n  prev_data.y = data.y;\n  prev_data.z = data.z;\n  if (head_ptr &gt;= SEQUENCE_LENGTH) {\n    head_ptr = 0;\n  }\n  if (head_ptr % inference_trigger_samples_num == 0) {\n    infer = true;\n  }\n}",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**Magic Wand via Gesture Recognition**</span>"
    ]
  },
  {
    "objectID": "contents/core/7_gesturerecog_magicwand.html#data-transfer-and-ble-communication",
    "href": "contents/core/7_gesturerecog_magicwand.html#data-transfer-and-ble-communication",
    "title": "8  Magic Wand via Gesture Recognition",
    "section": "8.5 Data Transfer and BLE Communication",
    "text": "8.5 Data Transfer and BLE Communication\nPredicted data is transferred through BLE using the GATT server as well as logged to the Putty terminal.\nvoid send_gesture_via_ble(uint8_t gesture)\n{\n    printf(\" BLE sent byte: %u\\r\\n\", (unsigned int)gesture);\n    sl_status_t sc;\n    sc = sl_bt_gatt_server_notify_all(gattdb_gesture_data,\n                                        sizeof(gesture),\n                                        &gesture);\n    app_assert_status(sc);\n}",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**Magic Wand via Gesture Recognition**</span>"
    ]
  },
  {
    "objectID": "contents/core/8_anomalydetection_htm.html",
    "href": "contents/core/8_anomalydetection_htm.html",
    "title": "9  IMU Anomaly Detection Using Hierarchical Temporal Memory",
    "section": "",
    "text": "9.1 System Overview\nHierarchical Temporal Memory (HTM) is an algorithm that mimics the brain’s neocortical learning mechanisms and constantly learns time-based patterns in unlabeled data. Anomalies (suspicious data) are flagged when data (IMU data) deviate significantly from learned patterns. In addition to the firmware, a Python script (display_serial.py) is written to monitor patterns and deviations.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**IMU Anomaly Detection Using Hierarchical Temporal Memory**</span>"
    ]
  },
  {
    "objectID": "contents/core/8_anomalydetection_htm.html#data-collection-and-preprocessing",
    "href": "contents/core/8_anomalydetection_htm.html#data-collection-and-preprocessing",
    "title": "9  IMU Anomaly Detection Using Hierarchical Temporal Memory",
    "section": "9.2 Data Collection and Preprocessing",
    "text": "9.2 Data Collection and Preprocessing\nLike in Chapter 7, the IMU captures accelerometer data in real-time at a frequency of 25Hz. The previous and current readings are processed to normalize motion along the x, y, and z axes into a \\([-1, 1]\\) range as shown below:\nimu_data_normalized.x = imu_data_current.x - imu_data_prev.x;\nimu_data_normalized.y = imu_data_current.y - imu_data_prev.y;\nimu_data_normalized.z = imu_data_current.z - imu_data_prev.z;\n\nimu_data_normalized.x /= 4000;\nimu_data_normalized.y /= 4000;\nimu_data_normalized.z /= 4000;",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**IMU Anomaly Detection Using Hierarchical Temporal Memory**</span>"
    ]
  },
  {
    "objectID": "contents/core/8_anomalydetection_htm.html#htm-model-architecture",
    "href": "contents/core/8_anomalydetection_htm.html#htm-model-architecture",
    "title": "9  IMU Anomaly Detection Using Hierarchical Temporal Memory",
    "section": "9.3 HTM Model Architecture",
    "text": "9.3 HTM Model Architecture\nThe IMU data is encoded into Sparse Distributed Representations (SDRs) to facilitate efficient anomaly detection.\n\n\n\n\n\n\n\nFigure 8.1: Example of IMU Anomaly\n\n\nsl_htm_encoder_simple_number(imu_data_normalized.x, -1.0f, 1.0f, 9, &sdr_x);\nsl_htm_encoder_simple_number(imu_data_normalized.y, -1.0f, 1.0f, 9, &sdr_y);\nsl_htm_encoder_simple_number(imu_data_normalized.z, -1.0f, 1.0f, 9, &sdr_z);\n\nsl_htm_sdr_insert(&input_sdr, &sdr_x, 0, SDR_WIDTH / 3 * 0);\nsl_htm_sdr_insert(&input_sdr, &sdr_y, 0, SDR_WIDTH / 3 * 1);\nsl_htm_sdr_insert(&input_sdr, &sdr_z, 0, SDR_WIDTH / 3 * 2);",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**IMU Anomaly Detection Using Hierarchical Temporal Memory**</span>"
    ]
  },
  {
    "objectID": "contents/core/8_anomalydetection_htm.html#visualization-and-real-time-monitoring",
    "href": "contents/core/8_anomalydetection_htm.html#visualization-and-real-time-monitoring",
    "title": "9  IMU Anomaly Detection Using Hierarchical Temporal Memory",
    "section": "9.4 Visualization and Real-Time Monitoring",
    "text": "9.4 Visualization and Real-Time Monitoring\nThe Python script display_serial.py visualizes anomaly scores in real-time by reading from a serial port connected to the microcontroller at a baudrate=115200. The script maintains a rolling buffer of scores and updates a live plot. This script helps visualize identified anomalies, such as irregular vibrations or sudden movements.\nif line.startswith(\"anom_score\"):\n    line_info = line.split(\":\")\n    anomaly_score = float(line_info[1])\n    buffer.append(anomaly_score)\n    buffer = buffer[1:]\n    axs.plot(buffer, color=\"red\", linewidth=1)\n    fig.tight_layout()\n    fig.canvas.draw()\n    plt.pause(0.001)\n    axs.clear()",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**IMU Anomaly Detection Using Hierarchical Temporal Memory**</span>"
    ]
  },
  {
    "objectID": "contents/core/9_audioml.html",
    "href": "contents/core/9_audioml.html",
    "title": "10  Audio ML for EFR32",
    "section": "",
    "text": "10.1 Overview\nIn the implementation below, the system processes raw audio input and categorizes it as either Yes or No using the trained ML model. The workflow includes three key stages:\nThe explanations and code examples are presented below for clarity. The required concepts are as follows:",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Audio ML for EFR32**</span>"
    ]
  },
  {
    "objectID": "contents/core/9_audioml.html#overview",
    "href": "contents/core/9_audioml.html#overview",
    "title": "10  Audio ML for EFR32",
    "section": "",
    "text": "Training the ML model using TensorFlow.\nConverting the model to a TensorFlow Lite format.\nDeploying the model on the EFR32 MCU for real-time inference.\n\n\n\nTensorFlow: An open-source framework widely used for developing and deploying machine learning models across platforms, including mobile and embedded systems. TensorFlow provides tools for building, training, and optimizing neural networks, along with TensorFlow Lite for Microcontrollers, which allows ML models to run efficiently on resource-constrained devices.\nAudio Features: Raw audio data, typically represented as waveforms, is transformed into meaningful numerical representations suitable for ML models. Commonly used features include Mel Frequency Cepstral Coefficients (MFCCs), which capture the spectral properties of audio signals, and Spectrograms, which represent the frequency content over time. These features enable neural networks to identify patterns and classify audio inputs accurately.\nEdge ML: Optimization of ML models for performance and memory efficiency on embedded devices.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Audio ML for EFR32**</span>"
    ]
  },
  {
    "objectID": "contents/core/9_audioml.html#training-the-model-in-tensorflow",
    "href": "contents/core/9_audioml.html#training-the-model-in-tensorflow",
    "title": "10  Audio ML for EFR32",
    "section": "10.2 Training the Model in TensorFlow",
    "text": "10.2 Training the Model in TensorFlow\n\n10.2.1 Preparing the Data\nLabeled audio clips containing the words Yes and No are required for training a model. A pre-recorded audio dataset, available in WAV format, will be used in this example. These audio files are first loaded, processed to extract MFCC features, and splitted into training and test sets.\nimport tensorflow as tf\nimport librosa\nimport numpy as np\nimport os\n\n# Extract MFCC features from an audio file\ndef extract_mfcc(file_path):\n    y, sr = librosa.load(file_path, sr=None)  # Load the audio file\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Extract MFCCs\n    return np.mean(mfcc, axis=1)  # Return the average of the MFCCs\n\n\n# Dataset preparation\ndef prepare_dataset(audio_dir):\n    features = []\n    labels = []\n    for label in ['yes', 'no']:\n        for file in os.listdir(os.path.join(audio_dir, label)):\n            if file.endswith('.wav'):\n                file_path = os.path.join(audio_dir, label, file)\n                mfcc_features = extract_mfcc(file_path)\n                features.append(mfcc_features)\n                labels.append(0 if label == 'no' else 1)  # 0 for \"No\", 1 for \"Yes\"\n    return np.array(features), np.array(labels)\n\nX, y = prepare_dataset('data/audio')\n\n# Splitting dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n10.2.2 Training the Neural Network Model\nA neural network is defined for binary classification of audio features.\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(13,)),  # 13 MFCC features\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n\n# Save the trained model\nmodel.save('yes_no_model.h5')",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Audio ML for EFR32**</span>"
    ]
  },
  {
    "objectID": "contents/core/9_audioml.html#converting-the-model-for-mcu-deployment",
    "href": "contents/core/9_audioml.html#converting-the-model-for-mcu-deployment",
    "title": "10  Audio ML for EFR32",
    "section": "10.3 Converting the Model for MCU Deployment",
    "text": "10.3 Converting the Model for MCU Deployment\nThe trained TensorFlow model is converted into TensorFlow Lite (TFLite) format for efficient deployment on resource-constrained devices.\nmodel = tf.keras.models.load_model('yes_no_model.h5')\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model\nwith open('yes_no_model.tflite', 'wb') as f:\n    f.write(tflite_model)\nThe conversion produces a ‘.tflite’ file suitable for embedded deployment.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Audio ML for EFR32**</span>"
    ]
  },
  {
    "objectID": "contents/core/9_audioml.html#implementing-the-model-on-the-efr32xg24",
    "href": "contents/core/9_audioml.html#implementing-the-model-on-the-efr32xg24",
    "title": "10  Audio ML for EFR32",
    "section": "10.4 Implementing the Model on the EFR32xG24",
    "text": "10.4 Implementing the Model on the EFR32xG24\nThe TensorFlow Lite model is integrated into the EFR32xG24 environment using the appropriate software development kit (SDK) and TensorFlow Lite for Microcontrollers library.\n\n10.4.1 Setting Up the Development Environment\nThe following components are required for setting up the development environment:\n\nEFR32xG24 SDK: The latest version of the Silicon Labs Gecko SDK must be installed.\nTensorFlow Lite for Microcontrollers: This library should be set up within the development environment.\n\n\n\n10.4.2 Loading the Model and Running Inference\nThe TensorFlow Lite model is loaded onto the MCU, input data is prepared, and inference is performed.\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\n#include \"tensorflow/lite/model.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n#include \"tensorflow/lite/kernels/register.h\"\n\n#define INPUT_SIZE 13\n\n// Declare tensors and interpreter\ntflite::MicroInterpreter* interpreter;\ntflite::Model* model;\ntflite::MicroAllocator* allocator;\n\nfloat input_data[INPUT_SIZE];  // Input data (MFCCs)\nfloat output_data[1];  // Output data (prediction)\n\n\n// Load the TensorFlow Lite model\nvoid LoadModel(const uint8_t* model_data) {\n    model = tflite::GetModel(model_data);\n    tflite::ops::micro::RegisterAllOps();\n    tflite::MicroInterpreter interpreter(model, tensor_arena, kTensorArenaSize, &resolver, &allocator);\n    interpreter.AllocateTensors();\n}\n\n// Perform audio classification\nint ClassifyAudio(float* mfcc_input) {\n    // Copy MFCC data into the input tensor\n    memcpy(interpreter.input(0)-&gt;data.f, mfcc_input, sizeof(float) * INPUT_SIZE);\n    \n    // Perform inference\n    interpreter.Invoke();\n\n    // Get the output prediction\n    float prediction = interpreter.output(0)-&gt;data.f[0];\n\n    // Return classification result: 1 for \"Yes\", 0 for \"No\"\n    return prediction &gt; 0.5 ? 1 : 0;\n\n}\nThe inference results are interpreted as:\n\n1: Detected Yes\n0: Detected No\n\n\n\n10.4.3 Integrating Audio Capture\nAn onboard microphone or an external microphone is often used to interface with the MCU to process real-time audio input. The EFR32xG24 does not include a dedicated audio processing block, requiring integration with a microphone module that outputs either analog signals (such as those from an electret microphone) or digital signals (like those from an I2S microphone). For simplicity, an analog microphone with an ADC on the MCU is employed here, with audio signals sampled, preprocessed, and then classified using the following steps:\n\nConfigure the ADC to sample audio signals.\nCapture the raw samples from the ADC at a suitable rate (e.g., 16 kHz or 8 kHz, depending on requirements).\nPreprocess the audio to extract features such as MFCC (Mel-frequency cepstral coefficients), which are suitable for ML models.\nFeed these features into the model for classification.\n\nHere is an example of how to collect and process audio data using an ADC for feature extraction using the EFR32 SDK.\n#include \"em_device.h\"\n#include \"em_chip.h\"\n#include \"em_adc.h\"\n#include \"em_cmu.h\"\n#include \"em_gpio.h\"\n#include \"em_interrupt.h\"\n\n// ADC buffer for storing captured samples\n#define BUFFER_SIZE 1024\nstatic uint16_t adc_buffer[BUFFER_SIZE];\nstatic volatile uint32_t adc_index = 0;  // Index for storing samples in the buffer\n\n// ADC interrupt handler to collect samples\nvoid ADC0_IRQHandler(void) {\n    // Read the ADC data from the ADC data register\n    adc_buffer[adc_index++] = ADC_DataSingleGet(ADC0);\n\n    // If the buffer is full, stop the ADC conversion\n    if (adc_index &gt;= BUFFER_SIZE) {\n        ADC0-&gt;CMD = ADC_CMD_STOP;\n        adc_index = 0;\n    }\n}\n\n// Initialize the ADC for audio sampling\nvoid ADC_InitAudio(void) {\n    // Enable the clock for ADC and GPIO\n    CMU_ClockEnable(cmuClock_ADC0, true);\n    CMU_ClockEnable(cmuClock_GPIO, true);\n\n    // Configure the GPIO pin for the microphone (assuming it is connected to a pin, e.g., PA0)\n    GPIO_PinModeSet(gpioPortA, 0, gpioModeInput, 0);\n\n    // Configure the ADC to sample at a reasonable rate for audio (e.g., 16 kHz)\n    ADC_Init_TypeDef adcInit = ADC_INIT_DEFAULT;\n    adcInit.prescale = ADC_PrescaleCalc(16000, 0);  // Calculate prescaler for 16kHz sampling rate\n    ADC_Init(ADC0, &adcInit);\n\n    // Configure the ADC single conversion mode\n    ADC_InitSingle_TypeDef adcSingleInit = ADC_INITSINGLE_DEFAULT;\n    adcSingleInit.input = adcSingleInputPin0;  // Set the input channel (e.g., PA0)\n    adcSingleInit.acqTime = adcAcqTime4;       // Set acquisition time\n    ADC_InitSingle(ADC0, &adcSingleInit);\n\n    // Enable the ADC interrupt and start ADC conversions\n    NVIC_EnableIRQ(ADC0_IRQn);\n    ADC0-&gt;CMD = ADC_CMD_START;\n}\n\n\n10.4.4 Audio Preprocessing and Classification\nThe audio data is stored in an array adc_buffer where the ADC samples are placed at regular intervals following these steps:\n\nADC samples the microphone data at a fixed rate.\nThe interrupt service routine (ISR) will be triggered each time the ADC completes a conversion.\nThe ISR stores the data into the adc_buffer.\n\nOnce the raw ADC samples are in the buffer, preprocessing is needed for the ML model. An example is as follows:\n\nConvert the ADC samples to a window of audio (e.g., 25 ms).\nApply a Fourier transform to convert the time-domain signal to the frequency domain.\nExtract MFCC features from the frequency-domain signal.\n\nAn example is provided on how to use the buffer data to classify using a Tensorflow Lite model.\nvoid ProcessAudioAndClassify() {\n    // Preprocess the raw ADC samples (simplified; actual MFCC extraction would be more complex)\n    float mfcc_input[INPUT_SIZE];  // Assumed 13 MFCC features\n    \n    // For simplicity, the ADC data is copied directly into the input array\n    // In a real case, processing is required (e.g., via FFT and MFCC extraction)\n\n    float mfcc_input[INPUT_SIZE];\n    for (int i = 0; i &lt; INPUT_SIZE; i++) {\n        mfcc_input[i] = (float)adc_buffer[i];\n    }\n\n    // Run the model to classify the audio\n    int prediction = ClassifyAudio(mfcc_input);\n    if (prediction == 1) {\n        printf(\"Detected: Yes\\n\");\n    } else {\n        printf(\"Detected: No\\n\");\n    }\n}\nThe main loop manages continuous audio capture and classification.\nint main(void) {\n    CHIP_Init();\n    ADC_InitAudio();\n\n    while (1) {\n        ProcessAudioAndClassify();\n        EMU_EnterEM1();\n    }\n}\n\n\n10.4.5 Considerations for Optimization\n\nADC Resolution: The ADC resolution and sampling rate must align with audio requirements.\nMFCC Extraction: Complex preprocessing, such as Fourier Transform and MFCC extraction, may require optimizations.\nPerformance: Model complexity and sampling rates should be adjusted for available memory and processing capabilities.",
    "crumbs": [
      "Embedded Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Audio ML for EFR32**</span>"
    ]
  }
]